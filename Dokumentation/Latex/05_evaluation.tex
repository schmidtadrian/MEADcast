\chapter{Evaluation} % (fold)
\label{chap:Evaluation}
Following the implementation (see \autoref{chap:Implementation}) and conduction
    of our experiment, \autoref{sec:Results} presents the results obtained from
    our measurements.
Subsequently, \autoref{sec:Discussion} discusses the implication of our
    findings, aiming to address the research questions formulated in
    \autoref{sec:Measurements}.

\section{Results} % (fold)
\label{sec:Results}
This section presents the results of our experiments.
\autoref{sub:Results_Performance} provides a comparison of performance metrics
    among IP Unicast, \gls{mead}, and IP Multicast, encompassing total network
    bandwidth utilization, sender upstream bandwidth utilization, and total
    transfer time.
\autoref{sub:Results_Grouping and Network support} delves into the implications
    of different sender configurations and varying levels of \gls{mead} network
    support on these performance metrics, as well as link bandwidth
    utilization, and latency.
In \autoref{sub:Results_Effects of the Discovery phase}, we examine the effect
    of the discovery phase on metrics such as bandwidth utilization, latency,
    and packet loss.
Lastly, \autoref{sub:Results_MEADcast in dynamic network environments} depicts
    the impact of events commonly observed in dynamic network environments,
    such as route change, link outages, and intermediate nodes dropping packets
    on \gls{mead} delivery.
\subsection{Performance Metrics} % (fold)
\label{sub:Results_Performance}
The conducted experiments consistently demonstrated that \gls{mead} achieved
    better results in all metrics compared to Unicast.
Across all \gls{mead} configurations tested, there was a significant reduction in
    total network bandwidth utilization, sender upstream bandwidth utilization,
    and transmission time.

\paragraph{Total Network Bandwidth} % (fold)
On average, the total network bandwidth was reduced by
    50.29\% in \nameref{par:EX1: Live Stream} (1 MB/s for 45s),
    58.78\% in \nameref{par:EX1: Live Stream} (2.5 MB/s for 45s), and
    58.93\% in \nameref{par:EX2: File Transfer} (25 MB)
    compared to IP Unicast (see \autoref{tab:rel_save_net_bw}).
Moreover, as the number of clients per router increased from one to five, the
    savings for 
    \uci{} and
    \ucii{}
    rose in total by 25\%  and 30\% respectively.
The relative reduction remained constant across varying data volumes.
% In comparison, IP Multicast achieved an average reduction of 80\%, with a
%     similar improvement observed as the number of receivers increased.
In comparison, IP Multicast achieved an average reduction of 80\% in network
    bandwidth compared to IP Unicast.
As the number of clients per router increased from one to five, the savings
    rose in total by 30\%, from 60\% for one client per router up to over
    90\% for five clients per router.

\begin{table}[h]
    \centering
        \begin{tabular}[c]{crrrrrr}
            \toprule
            \multirow{2}{*}[-2pt]{\makecell[c]{\textbf{Clients}\\\textit{(per Router)}}}
            & \multicolumn{2}{c}{\textbf{UC1} \textit{(1 MB/s)}}
            & \multicolumn{2}{c}{\textbf{UC1} \textit{(2.5 MB/s)}}
            & \multicolumn{2}{c}{\textbf{UC2} \textit{(25 MB)}} \\
            \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
            &
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} & 
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} & 
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} \\
            \midrule
            1       & 25.50\%    & 59.59\%    & \makecell[c]{-}    & \makecell[c]{-}   & 42.11\%           & 64.69\%         \\
            2       & 49.20\%    & 79.79\%    & 50.13\%            & 79.76\%           & \makecell[c]{-}   & \makecell[c]{-} \\
            3       & 56.43\%    & 86.53\%    & 53.17\%            & 84.29\%           & 63.30\%           & 88.24\%         \\
            4       & 60.28\%    & 89.89\%    & 60.34\%            & 86.35\%           & \makecell[c]{-}   & \makecell[c]{-} \\
            5       & 60.06\%    & 91.45\%    & 71.49\%            & 88.62\%           & 71.38\%           & 94.24\%         \\
            \midrule
            \textbf{Average} & 50.29\%    & 81.45\%    & 58.78\%            & 84.76\%           & 58.93\%           & 82.39\%         \\
            \bottomrule
        \end{tabular}
    \caption[Total network bandwidth reduction]{Total network bandwidth reduction \textit{(relative to unicast)}}
    \label{tab:rel_save_net_bw}
\end{table}

\paragraph{Sender Upstream Bandwidth} % (fold)
Furthermore, we observed a significant decrease of the sender upstream
    bandwidth consumption compared to IP Unicast.
On average, it was reduced by
    78.06\% in \uci{} (1 MB/s),
    81.03\% in \uci{} (2.5 MB/s), and
    84.61\% in \ucii{} (25 MB)
    (see \autoref{tab:rel_save_upstream}).
In comparison, IP Multicast achieved relative reductions of 96.71\%, 97.33\%,
    and 94.41\%, respectively.
Similar to the findings for the total network bandwidth, the relative reduction
    remained constant across various data volumes.
As the number of clients per router increased from one to five, the savings
    for \uci{} (1 MB/s) rose by 15\% from
    64.61\% for one client per router up to 80.95\% for five clients.


\begin{table}[h]
    \centering
        \begin{tabular}[c]{crrrrrr}
            \toprule
            \multirow{2}{*}[-2pt]{\makecell[c]{\textbf{Clients}\\\textit{(per Router)}}}
            & \multicolumn{2}{c}{\textbf{UC1} \textit{(1 MB/s)}}
            & \multicolumn{2}{c}{\textbf{UC1} \textit{(2.5 MB/s)}}
            & \multicolumn{2}{c}{\textbf{UC2} \textit{(25 MB)}} \\
            \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
            &
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} & 
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} & 
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} \\
            \midrule
            1       & 64.61\%    & 92.90\%    & \makecell[c]{-}     & \makecell[c]{-}     & 79.84\%            & 89.03\%         \\
            2       & 78.12\%    & 96.47\%    & 78.08\%             & 96.45\%             & \makecell[c]{-}    & \makecell[c]{-} \\
            3       & 81.97\%    & 97.67\%    & 80.81\%             & 97.37\%             & 87.65\%            & 96.36\%         \\
            4       & 84.65\%    & 98.22\%    & 84.77\%             & 97.86\%             & \makecell[c]{-}    & \makecell[c]{-} \\
            5       & 80.95\%    & 98.31\%    & 80.45\%             & 97.64\%             & 86.33\%            & 97.86\%         \\
            \midrule
            \textbf{Average} & 78.06\% & 96.71\%   & 81.03\%             & 97.33\%             & 84.61\%            & 94.41\%         \\
            \bottomrule
        \end{tabular}
    \caption[Total sender upstream bandwidth reduction]{Total sender upstream bandwidth reduction \textit{(relative to unicast)}}
    \label{tab:rel_save_upstream}
\end{table}

\paragraph{Transfer Time}
Our experiments revealed significant reductions in total transfer time with
    \gls{mead}.
On average, \gls{mead} reduced the total transfer time by
    8.94\%  in \uci{} (1 MB/s),
    8.93\%  in \uci{} (2.5 MB/s), and
    24.16\% in \ucii{} (25 MB).
This emphasizes that, given \gls{mead}'s lower bandwidth consumption,
    transfer time can be significantly reduced in scenarios where network
    or sender upstream bandwidth poses a bottleneck.

However, it is crucial to acknowledge that the observed time savings in \uci{},
    which is fixed to 45 seconds of data transmission, may be influenced by CPU
    limitations.
As the unicast transmission requires, one \textit{``Iperf''} instance per
    receiver, scheduling significantly more \textit{``Iperf''} processes than
    available CPUs is necessary for high numbers of receivers.
The fact, that notable time savings in \uci{} only occur for a high number of
    clients supports this hypothesis.
In contrast, \ucii{} consistently exhibits reduced transfer times across all
    numbers of clients.
Consequently, our subsequent discussion will focus on the results from \ucii{}.

\autoref{fig:trans_time} illustrates that \gls{mead} possesses a significantly
    lower growth rate in transfer time, as the number of group members
    increases compared to IP Unicast.
It scales more similarly to IP Multicast in our experiments.
For instance, with 5 clients per router, Multicast reduced the transfer time
    by 77.97\%, from 2 minutes and 46 seconds to 36 seconds, while \gls{mead} 
    achieved a reduction of 57.46\%, resulting in a transfer time of 1 minute
    and 11 seconds (see \autoref{tab:rel_save_time}).


% In file transfer experiment we verified, that transfering different sizes (25
%     \& 100 MB) results in the same relative savings.
% Therefore we tested with 25 MB. 

% General stats
% Impact Discovery
%   - Performs (netload, upstream, transmission time) only slightly (<5%) worse
%     than non discovery equivivalent
%   - Not noticiable in Jitter
%   - Noticiable in AVG Latency for higher number of clients (why?)
%   - Discovery Phase increases MAX latency (why?)
%   - No peaks in netload or upstream recognizable 
% Grouping
%   - Grouping does strongly influence performance
%   - The more addresses per packet the lower the upstream bandwidth
%   - (1) L2L3, (2) plain, (3) merge, L1
%   - In both experiments MEADcast support only on L2L3 results on certain
%     links in bandwidth savings of over 50%, because MEADcast router replicate
%     packets eventhough they travel the same path
% Dynamic network environments
%   - Turn on FW: 100% packet loss until next discovery phase (for us ca. 2s).
%     If falling back to a MEADcast router before the firewall, we experienced
%     an increase in netload bc of the earlier converstion from m2u.
%     If falling back to unicast way higher increase compared to falling back
%     to MEADcast. After allowing MEADcast traffic on FW again and next discovery
%     phase, back to normal.%
%   - Routing change: Not recognizable in RX BW and no packets lost. Higher latency.
%     Same behavior as Unicast.
%   - Link failure: RX BW drops to zero. After next discovery phase Link BW
%     increased bc of less optimal grouping. After routing change RX BW back up
% Always performed better than unicast and worse than multicast, no matter of
% config.
% We can see  that in tables ...

% Network bandwidth:
% - 56%
% - 82.72%
% Sender upstream:
% - 81.23%
% - 96.08%
% Transfer time:
% - 12.03% (24.16%)
% - 27.40% (49.18%)

% Conclusion:
% - less MEADcast support at the right spots performs better than 100%



\begin{table}[h]
    \centering
        \begin{tabular}[c]{crrrrrr}
            \toprule
            \multirow{2}{*}[-2pt]{\makecell[c]{\textbf{Clients}\\\textit{(per Router)}}}
            & \multicolumn{2}{c}{\textbf{UC1} \textit{(1 MB/s)}}
            & \multicolumn{2}{c}{\textbf{UC1} \textit{(2.5 MB/s)}}
            & \multicolumn{2}{c}{\textbf{UC2} \textit{(25 MB)}} \\
            \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
            &
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} & 
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} & 
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} \\
            \midrule
            1       & 0.65\%     & 0.66\%     & \makecell[c]{-}    & \makecell[c]{-}    & 37.94\%            & 61.54\%         \\
            2       & 1.11\%     & 1.10\%     & 2.93\%             & 2.90\%             & \makecell[c]{-}    & \makecell[c]{-} \\
            3       & 2.27\%     & 2.27\%     & 18.43\%            & 18.55\%            & 52.14\%            & 62.50\%         \\
            4       & 10.39\%    & 10.40\%    & 36.19\%            & 36.24\%            & \makecell[c]{-}    & \makecell[c]{-} \\
            5       & 30.30\%    & 30.22\%    & 38.82\%            & 38.94\%            & 57.46\%            & 77.97\%         \\
            \midrule
            % Average & 3.00\% &	8.94\%     & 8.93\%             & 24.09\%            & 24.16\%            &	49.18\% \\
            \textbf{Average} & 8.94\%     & 8.93\%     & 24.09\%            & 24.16\%            & 49.18\%            & 67.34\%         \\

            \bottomrule
        \end{tabular}
    \caption[Total transfer time reduction]{
        Total transfer time reduction \textit{(relative to unicast)}
        The reduction observed in \uci{} is primarily attributed to CPU limitations.
        This is because, in IP Unicast measurements with a high number of clients,
        the sender schedules significantly more \textit{``Iperf''} processes than
        available cores.
    }
    \label{tab:rel_save_time}
\end{table}

\begin{figure}
    \begin{center}
        \input{Bilder/line_ex2_25mb_tx_time.pgf}
    \end{center}
    \caption[UC2 File transfer: Comparison of transfer time]{
        \nucii{} Comparison of total transfer time for a data volume of 25MB.
    }
    \label{fig:trans_time}
\end{figure}


% Fig X illustrates these metrics during two experiments.
% Network bandwidth mehr als halbiert (64%, 71%).
% Upstream um  (87%, 92%)
% Tansmit time (10%, 55%)
% Initial discovery klar zu erkennen
% UC1 stready, UC2 higher variation bc operating on limit
\autoref{fig:netload_upstream_cmp}, illustrates the performance enhancements in
    two specific scenarios, with \uci{} on the left-hand side and \ucii{} on
    the right.
In both cases, \gls{mead} substantially reduces network bandwidth utilization,
    achieving reductions of over half compared to IP Unicast (\uci{}: 64\%,
    \ucii{}: 71\%) (see \autoref{fig:netload_upstream_cmp} (a) and (c)).
Moreover, \gls{mead} achieves even greater reductions in sender upstream
    bandwidth utilization, reaching 87\% in \uci{} and 92\% in \ucii{} compared
    to IP unicast (see \autoref{fig:netload_upstream_cmp} (b) and (d)).
At the start of \gls{mead} transmission, there is a discernible impact from the
    initial discovery phase.
During this phase, both network and upstream bandwidth experience peaks for
    approximately 2 seconds, reaching similar values as IP Unicast, and even
    surpassing them in \ucii{}.
The bandwidth utilization in \ucii{} fluctuates more compared to \uci{} because
    it pushes the testbed's resource usage to its limits.
In \ucii{}, \gls{mead} significantly reduces transmission time by 55\% compared to
    IP Unicast (see \autoref{fig:netload_upstream_cmp} (c) and (d)).

\begin{figure}
    \begin{center}
        \input{Bilder/line_netload_upstream.pgf}
    \end{center}
    \caption[Comparison of network and sender upstream bandwidth utilization]{
        Comparison of network and sender upstream bandwidth utilization.
        On the left side, \textbf{(a)} depicts the total network bandwidth
        utilization and \textbf{(b)} the sender upstream bandwidth utilization
        during the execution of \nuci{} (1 MB/s for 45s) with 56 clients
        (5 per router), a maximum of 20 addresses per packet, and a discovery
        interval of 5 seconds.
        On the right side, \textbf{(c)} represents the total network bandwidth
        utilization and \textbf{(d)} the sender upstream bandwidth utilization
        during \nucii{} (25 MB) with 45 clients (5 per router), a
        maximum of 24 addresses per packet and a discovery interval of 5
        seconds.
    }
    \label{fig:netload_upstream_cmp}
\end{figure}
% section Performance (end)



\subsection{Grouping and Network support} % (fold)
\label{sub:Results_Grouping and Network support}

We conducted tests on \gls{mead} using various groupings and different levels of
    \gls{mead} network support.
As previously noted, \gls{mead} consistently achieved better results in all
    metrics compared to IP Unicast across all tested groupings and levels of
    network support.
However, we observed variations in our findings depending on the \gls{mead}
    configuration.

\subsubsection{Bandwidth Utilization}
Interestingly, the most significant reduction in total network bandwidth
    utilization was not achieved with 100\% network support, but rather with
    \gls{mead} support only on the Distribution (L2) and Access Layer (L3)\footnote{
        \autoref{sec:Hierachical Three-Layer Internet Model} introduces the terms Core
            (L1), Distribution (L2), and Access Layer (L3).
        \autoref{sub:Network Topology} provides information about the corresponding
            layers in our topology.
    }, as shown in \autoref{fig:netloadcmp}.
This was closely followed by measurements with a merge range of 2\footnote{
        \autoref{sub:Grouping} provides information about the merge range parameter.
    }.
In contrast, experiments with \gls{mead} support only on the Core Layer (L1)
    achieved the least improvement compared to IP Unicast.
Each of these experiments was conducted with a maximum address list length of
    10 and 20.
Notably, in all tested experiments in \uci{}, a maximum address list of 20
    consistently achieved lower bandwidth utilizations than its counterpart
    with a limit of 10 addresses.
The drop in total network bandwidth utilization in \uci{} (2.5 MB/s) is caused
    by an increased packet loss rate due to the resource limitations of the
    testbed.

\begin{figure}
    \begin{center}
        \input{Bilder/line_ex1_netload.pgf}
    \end{center}
    \caption[UC1 Live stream: Total network bandwidth utilization]{
        \nuci{} Total network bandwidth utilization.
        \textbf{(1)} Red lines depict measurements with a maximum address list length of 10.
        \textbf{(2)} Blue lines depict measurements with a maximum address list length of 20.
        \textbf{(3)} Line style indicates certain \gls{mead} configuration, for
            instance dashed lines represent measurements with network support
            on layer 2 and 3.
        \textbf{(4)} \textit{Mead 10/20}:
            Utilizes a maximum address list length of 10 and 20 respectively,
            with 100\% network support.
        \textbf{(5)} \textit{Mead 10/20 M2}:
            Same as \textit{Mead 10/20} with a merge range of 2.
        \textbf{(6)} \textit{Mead 10/20 L2L3/L1}:
            Same as \textit{Mead 10/20} with network support limited to layer 2 and 3,
            and layer 1, respectively.
        }
    \label{fig:netloadcmp}
\end{figure}

\autoref{fig:upstreamcmp} illustrates, that all tested groupings and levels of
    network support exhibit significantly lower sender upstream bandwidth
    requirements compared to IP Unicast.
Furthermore, as the number of receivers increases, the upstream bandwidth
    demands of \gls{mead} grow at a much slower rate than IP Unicast.
Additionally, the higher the address list limit, the fewer packets with
    identical payloads are required, resulting in a greater reduction in sender
    upstream bandwidth utilization.
For instance, configurations with an address limit of 20 consistently
    achieved lower upstream bandwidth consumption than all configurations with
    a limit of 10.
In contrast to the reductions in total network bandwidth, experiments with
    \gls{mead} support exclusively on L2 and L3 achieved the least improvement.
Experiments with 100\% \gls{mead} support and a merging range between 1 and 2
    yielded comparable results.

\begin{figure}
    \begin{center}
        \input{Bilder/line_upstream.pgf}
    \end{center}
    \caption[Total sender upstream bandwidth]{
        Total sender upstream bandwidth.
        \textbf{(1)} Red lines depict measurements with a maximum address list length of 10.
        \textbf{(2)} Blue lines depict measurements with a maximum address list length of 20.
        \textbf{(3)} Line style indicates certain \gls{mead} configuration, for
            instance dashed lines represent measurements with network support
            on layer 2 and 3.
        \textbf{(4)} \textit{Mead 10/20}:
            Utilizes a maximum address list length of 10 and 20 respectively,
            with 100\% network support.
        \textbf{(5)} \textit{Mead 10/20 M2}:
            Same as \textit{Mead 10/20} with a merge range of 2.
        \textbf{(6)} \textit{Mead 10/20 L2L3/L1}:
            Same as \textit{Mead 10/20} with network support only on layer 2 and 3,
            and layer 1, respectively.
    }
    \label{fig:upstreamcmp}
\end{figure}

\autoref{fig:link_bw_l2l3_100} depicts a comparison of bandwidth usage per 
    link between measurements with 100\% \gls{mead} support and network support
    limited to L2 and L3.
Consistent with the observations regarding total network bandwidth utilization,
    \gls{mead} support exclusively on L2 and L3 leads to lower bandwidth
    consumption per link compared to 100\% network support.
% Continuing with the examination of the bandwidth savings resulting from
%     MEADcast support solely on L2 and L3, \autoref{fig:link_bw_l2l3_100}
%     highlights the bandwidth usage per link in experiments with 100\% MEADcast
%     support versus protocol support exclusively on L2 and L3.
In both \uci{} and \ucii{}, the lower level of \gls{mead} support notably
    decreases bandwidth usage on certain links by over 50\%.

    
Furthermore, we executed \ucii{} with 27 receivers (three per router), a
    maximum of 30 addresses per packet, and \gls{mead} support limited to a
    single router, $R_3$.
This experiment resulted in a reduction in transfer time by 42.98\%, network
    bandwidth utilization by 53.14\%, and sender upstream bandwidth utilization
    by 94.55\%.

\begin{figure}
    \begin{center}
        \input{Bilder/hbar_link_bw_l2l3_100.pgf}
    \end{center}
    \caption[UC2 File Transfer: Comparison of total link bandwidth]{
        \nucii{} Comparison of total link bandwidth.
        \textbf{(1)} Black bars denote results from measurements with 100\%
            \gls{mead} support.
        \textbf{(2)} Red bars denote results from measurements with \gls{mead}
            support on the Distribution (L2) and Access Layer (L3).
        \textbf{(a)} depicts the results from the execution of \nuci{} (1 MB/s
            for 45 seconds), with 70 receivers (five per router),
            and a maximum of 20 addresses per packet.
        \textbf{(b)} depicts the results from the execution of \nucii{} (25
            MB), with 45 receivers (five per router), and a maximum of 24
            addresses per packet.
    }
    \label{fig:link_bw_l2l3_100}
\end{figure}

%%%% LATENCY & JITTER %%%%
% Results:
%   Latency:
%    - Discovery Latency is similar or slightly higher than Uni
%    - AVG Latency: uni lower for small groups, switches with increasing group size
%    - MIN Latency: Uni best,  Mead worst
%    - MAX Latency: small group size all similar, increasing group size uni and mead discover worse
% Discussion:
%   Uni rises, bc we reach resource limits
%   Min uni best, bc without resource constraints its the fastest
%   --> MEADcast can save resources and therefore work in settings where Uni is not feasible
%   AVG latency for discovery rises mostly bc of uni cast usage during init discovery phase (see line latency)

\subsubsection{Latency}
Both the grouping configuration and the level of \gls{mead} network support
    have implications for latency.
For small group sizes, \gls{mead} leads to a slight increase in average latency
    compared to IP Unicast (see \autoref{fig:latency_cmp} (a)).
However, as the number of receivers grows to 42 (three clients per router), the
    latency of IP Unicast significantly exceeds that of \gls{mead}.
\gls{mead} support limited to L2 and L3 exhibits the lowest latency among all
    tested \gls{mead} configurations, followed by 100\%.
% We measured the lowest latency with \gls{mead} support on L2 and L3 followed by
%     100\% network support.
Similar to IP Unicast, as the group size increases, \gls{mead} with periodic
    discovery phase exceeds the average latency of \gls{mead} without discovery.
However, it still remains lower than IP Unicast.
The maximal latency yields similar results, with the latency of \gls{mead} with
    discovery phase being similar to or slightly higher than that of IP Unicast
    (see \autoref{fig:latency_cmp} (c)).
% Analogous to IP Unicast, for large group sizes, \gls{mead} with periodic discovery
%     phases yielded a higher average latency compared to \gls{mead} without
%     discovery, however, it is still lower than IP Unicast.
% The maximal latency shows similar results, besides that the latency of \gls{mead}
%     with discovery phase is similar to or slightly higher than IP Unicast (see
%     \autoref{fig:latency_cmp} (c)).

The minimal latency of \gls{mead} is higher than that of IP Unicast (see
    \autoref{fig:latency_cmp} (b)).
\gls{mead} achieves results more akin to IP Multicast.
All tested \gls{mead} configurations with an address list limit of 20 possess
    lower minimal latency than any configuration with a limit of 10.
The increase in latency, as the number of receivers grows, is the highest for
    the maximal latency and the lowest for the minimal latency.
Throughout our experiments, we also measured average, minimal, and maximal
    Jitter.
However, the results indicate no measurable impact from \gls{mead} on Jitter.

\begin{figure}
    \begin{center}
        \input{Bilder/line_ex1_1mb_latency.pgf}
    \end{center}
    \caption[UC1 Live stream: Latency comparison]{
        \nuci{} Latency comparison.
        \textbf{(1)} Red lines depict measurements with a maximum address list length of 10.
        \textbf{(2)} Blue lines depict measurements with a maximum address list length of 20.
        \textbf{(3)} Line style indicates certain \gls{mead} configuration, for
            instance dashed lines represent measurements with network support
            on layer 2 and 3.
        \textbf{(4)} \textit{Mead 10/20}:
            Utilizes a maximum address list length of 10 and 20 respectively,
            with 100\% network support.
        \textbf{(6)} \textit{Mead 10/20 L2L3}:
            Same as \textit{Mead 10/20} with network support limited to layer 2 and 3.
    }
    \label{fig:latency_cmp}
\end{figure}

% section Grouping and Network support (end)


\subsection{Effects of the discovery phase} % (fold)
\label{sub:Results_Effects of the Discovery phase}

% higher latency for a large number of receivers
% spikes in latency: for small number of receivers nearly no increase in avg latency
% for larger number of receivers notable peaks in both avg and max latency
As previously demonstrated, when dealing with a large number of receivers,
    \gls{mead} with periodic discovery phase tends to exhibit higher latency
    compared to \gls{mead} without discovery phase.

\autoref{fig:dcvr_latency_effect}, illustrates spikes in maximum latency for
    both 28 and 70 receivers during discovery phases.
Additionally, there is a noticeable increase in average latency for 70 clients
    during these phases, although the increase is marginal for 28 clients.
Especially the initial discovery phase results in a significant spike in
    total network bandwidth utilization, sender upstream bandwidth utilization,
    and latency (see \autoref{fig:netload_upstream_cmp} for total network and
    sender upstream bandwidth utilization, and
    \autoref{fig:dcvr_latency_effect} for latency).
Moreover, if the network is already operating at its capacity limit, discovery
    phases, accompanied by an increased traffic volume, can result in elevated
    packet loss, as shown in \autoref{fig:dcvr_loss_effect}.
Notably, the initial discovery phase, accompanied by IP Unicast data
    transmission, has also the most substantial effect on packet loss.

Consequently, \autoref{tab:init_dcvr_latency} demonstrates the latency increase
    caused by the discovery phase, particularly focusing on the initial
    discovery.
In experiments with discovery phase, the average latency increases by 39.57\%.
Excluding the initial discovery phase and the associated IP Unicast
    transmission, the increase in latency is reduced by more than half, to
    15.69\%.
Furthermore, the results emphasize a distinct increase in latency as the number
    of clients rises.
For three or fewer clients per router, the latency increase is 11.1\% with 
    initial discovery and 3.9\% without.
However, for more than three clients per router, the latency increase grows to
    75.16\% with initial discovery and to 30.83\% without.

\autoref{tab:dcvr_net_up_effect} illustrates the impact of the discovery phase
    on network bandwidth utilization and sender upstream bandwidth utilization.
The discovery phase increases the network bandwidth utilization by an average
    of 4.69\%, while the sender upstream bandwidth utilization increases by
    18.36\% (compared to the corresponding experiment without discovery phase).
Excluding the initial discovery phase and the associated IP Unicast
    transmission, the increase in network bandwidth utilization reduces to
    0.62\%, and for sender upstream utilization, it decreases to 0.42\%.
The theoretically calculated increase in sender upstream bandwidth is merely
    0.04\%.
The average increase in total transfer time is 0.88\% for \ucii{} (25 MB).

\begin{figure}
    \begin{center}
        \input{Bilder/line_latency_cmp.pgf}
    \end{center}
    \caption[Effect of the discovery phase on latency]{
        \nuci{} Effect of the discovery phase on latency.
        The vertical dashed line represents the periodic discovery phase.
        The green line depicts the maximal latency and the blue line the
        average latency.
    }
    \label{fig:dcvr_latency_effect}
\end{figure}

\begin{figure}
    \begin{center}
        \input{Bilder/line_ex3_dcvr_loss.pgf}
    \end{center}
    \caption[Effect of the discovery phase]{
        Effect of the discovery phase.
        \textbf{(a)} depicts the received bandwidth on the sender.
        \textbf{(b)} depicts the packet loss on a receiver.
        \textbf{(c)} depicts the maximal latency on a receiver.
    }
    \label{fig:dcvr_loss_effect}
\end{figure}


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
\centering
\begin{tabular}{@{}ccrrrrrr@{}}
\toprule
    & & \multicolumn{4}{c}{\textbf{Average Latency} \textit{(ms)}} & \multicolumn{2}{c}{\textbf{Rel. increase} \textit{(\%)}} \\
    \cmidrule(lr){3-6}\cmidrule(lr){7-8}
        \makecell{\bfseries Clients\\\itshape (per Router)}
    &   \textbf{\makecell{Address\\limit}}
    &   \multicolumn{1}{c}{\makecell{No\\Dcvr}}
    &   \multicolumn{1}{c}{\makecell{Incl. init.\\Dcvr}}
    &   \multicolumn{1}{c}{\makecell{Excl. init.\\Dcvr}}
    &   \multicolumn{1}{c}{\makecell{Diff\\\itshape (\%)}}
    &   \multicolumn{1}{c}{\makecell{Incl. init.\\Dcvr}}
    &   \multicolumn{1}{c}{\makecell{Excl. init.\\Dcvr}} \\ \midrule
    1 & 10  &  2.77  & 2.72  & 2.69  & 0.86\%    & -2.17\%   & -3.05\%   \\
    2 & 10  &  3.14  & 3.81  & 3.54  & 6.93\%    & 17.51\%   & 11.37\%   \\
    3 & 10  &  4.31  & 4.51  & 4.20  & 7.01\%    & 4.40\%    & -2.81\%   \\
    4 & 10  &  5.15  & 24.16 & 6.42  & 73.42\%   & 78.70\%   & 19.86\%   \\
    5 & 10  &  12.42 & 59.28 & 33.98 & 42.67\%   & 79.05\%   & 63.46\%   \\ \midrule
    2 & 20  &  3.33  & 3.64  & 3.50  & 3.84\%    & 8.46\%    & 4.80\%    \\
    3 & 20  &  4.20  & 5.77  & 4.53  & 21.53\%   & 27.32\%   & 7.39\%    \\
    4 & 20  &  6.10  & 19.92 & 6.08  & 69.46\%   & 69.38\%   & -0.27\%   \\
    5 & 20  &  9.94  & 37.49 & 16.69 & 55.47\%   & 73.49\%   & 40.47\%   \\ \midrule
    & & & \multicolumn{2}{r}{\textbf{Average}}  & 31.24\%   & 39.57\%   & 15.69\% \\ \bottomrule
\end{tabular}
\caption[Latency increase caused by the discovery phase]{
    Latency increase caused by the discovery phase.
    We gathered the results by executing the measurements once with and once
        without periodic discovery phase.
    The relative difference describes the variation between the measurements
        conducted with and without the initial discovery phase.
    The relative increase refers to the corresponding experiment without
        discovery phase.
}
\label{tab:init_dcvr_latency}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
\centering
\begin{tabular}{@{}lrrrrrc@{}}
\toprule
    & & \multicolumn{5}{c}{\textbf{Bandwidth}} \\ \cmidrule(lr){3-7}
    & & \multicolumn{2}{c}{\textbf{\makecell{Inclusive initial\\discovery phase}}}
    & \multicolumn{3}{c}{\textbf{\makecell{Exclusive initial\\discovery phase}}} \\ \cmidrule(lr){3-4}\cmidrule(lr){5-7}
    \multicolumn{1}{c}{\textbf{Experiment}} &
    \multicolumn{1}{c}{\textbf{Time}} &
    \multicolumn{1}{c}{Network} &
    \multicolumn{1}{c}{Sender} &
    \multicolumn{1}{c}{Network} &
    \multicolumn{1}{c}{Sender} &
    \multicolumn{1}{c}{Sender \textit{(Calc.)}\tnote{1}} \\ \midrule
    UC1 \textit{(1 MB/s)}                   & 0.00\%  & 6.17\%  & 17.37\%   & 0.54\%    & -2.24\%   & 0.06\% \\
    UC1 \textit{(2.5 MB/s)}                 & -0.03\% & 2.79\%  & 16.35\%   & 0.78\%    & 2.48\%    & 0.03\% \\
    UC2 \textit{(25 MB)}                    & 0.88\%  & 5.11\%  & 21.37\%   & 0.55\%    & 1.01\%    & 0.02\% \\ \midrule
    \multicolumn{1}{c}{\textbf{Average}}    & 0.28\%  & 4.69\%  & 18.36\%   & 0.62\%    & 0.42\%    & 0.04\% \\ \bottomrule
\end{tabular}
    \caption[Bandwidth utilization increase caused by the discovery phase]{
        Bandwidth utilization increase caused by the discovery phase.
        We gathered the results by executing the measurements once with and once
            without periodic discovery phase.
        100\% refers to the corresponding experiment without discovery phase.
    }
\label{tab:dcvr_net_up_effect}
\end{table}


Furthermore, we observed that in the current sender implementation, the order
    in which it receives discovery responses also affects the grouping of the
    clients.
However, we did not observe any performance implications of the change in
    receiver grouping.
% section Influence of the Discovery phase (end)

\subsection{MEADcast in dynamic network environments} % (fold)
\label{sub:Results_MEADcast in dynamic network environments}

% Fallback
% After enabeling FW, receive BW drops to 0, netload drops low
% After next discovery, client receive traffic again.
%   the net bw goes up (bc longer time or whole path uni)
% After disableing nothing happens in first place.
%   But after next discovery netload reduces, bc sender switches back to MEADcast
In \nuciii{} we evaluated \gls{mead}'s resilience and recovery mechanisms
    required for dynamic network environments.

We deliberately introduced a firewall to drop \gls{mead} packets during 
    transmission.
\autoref{fig:dyn_net_env} (a) and (b) illustrate, that upon enabling the firewall,
    receiver bandwidth drops to zero, accompanied by a decrease in network
    bandwidth utilization.
Following the subsequent discovery phase, the sender attempts to assign clients
    to a router located in front of the firewall.
If this is not feasible, the sender falls back to IP Unicast transmission.
Consequently, the packets traverse the firewall and the client receives traffic
    again.
Fallback to IP Unicast increases total network bandwidth utilization from 15
    MB/s to 50 MB/s, whereas falling back to another \gls{mead} router raises
    bandwidth only up to 28 MB/s.
Upon firewall deactivation, initially, no change is observed.
However, after the subsequent discovery phase, the sender reverts to the
    initial \gls{mead} grouping.
This results in the network bandwidth utilization returning to its pre-firewall
    activation level of 15 MB/s.

Furthermore, we assessed \gls{mead}'s resilience to route changes.
Despite altering routes during \gls{mead} transmission, receiver bandwidth
    remained consistent (see \autoref{fig:dyn_net_env} (d)).
The only observed effect was a peak in latency.
After the route change, packet delivery remained undisrupted as we
    redirected traffic from $R_1$ to $R_3$ (\textit{r1\_r3}) to traverse via
    $R_4$ (\textit{r1\_r4}), ensuring the packet keeps traversing all \gls{mead}
    routers from its address list (see \autoref{fig:dyn_net_env} (c)).
It is crucial to note that if a \gls{mead} packet does not pass through a router
    listed in its header, there is a risk of incorrect processing, potentially
    resulting in the packet not being delivered to its intended receivers. 


Lastly, we evaluated \gls{mead}'s recovery from a link outages.
Immediately after the link failure between $R_1$ and $R_3$, receiver
    bandwidth dropped to zero (see \autoref{fig:dyn_net_env} (f)).
Following the subsequent discovery phase, $R_1$'s output bandwidth
    (\textit{r1\_r3}) increased from 1.5 MB/s to 8 MB/s (see
    \autoref{fig:dyn_net_env} (e)).
This increase occurred due to a new grouping with an earlier \gls{m2u}
    transformation.
Once the underlying routing protocol detected the link failure and adjusted
    the route accordingly, clients resumed receiving traffic.
Similar to the route change experiment, the routing protocol redirected traffic
    from $R_1$ to $R_3$ (\textit{r1\_r3}) to traverse via $R_4$
    (\textit{r1\_r4}), ensuring packets resumed traversing all \gls{mead}
    routers from their address list, resulting in correct packet processing
    and delivery.
Following the subsequent discovery phase, the sender reverted to the initial
    grouping, restoring link bandwidth utilization to its pre-outage level of
    1.5 MB/s.

\begin{figure}
    \begin{center}
        \input{Bilder/line_ex3_p2p_dyn_net_env.pgf}
    \end{center}
    \caption[MEADcast fallback and recovery mechanism]{
        \nuciii{} \gls{mead} fallback and recovery mechanism:
        \textbf{(1)} The left column illustrates the total network bandwidth
            \textbf{(a)} and receiver bandwidth \textbf{(b)} in a scenario
            where \gls{mead} packets are dropped during transmission, for
            instance, due to firewall intervention.
        The black line represents the results when falling back to IP Unicast
            transmission.
        The red line illustrates the results from scenarios where the clients
            could be assigned to a \gls{mead} router located in front of the
            firewall.
        The triangular marker indicate the period during which \gls{mead} packets
            are dropped and the square marker represent the discovery phase.
        \textbf{(2)} The middle column illustrates the bandwidth per link
            \textbf{(c)} and receiver bandwidth \textbf{(d)} in a scenario
            where a route change occurred during transmission.
        \textbf{(3)} The right column  shows the bandwidth per link
            \textbf{(e)} and receiver bandwidth \textbf{(f)} in a scenario
            where a link outage occurred during transmission.
        The square marker indicate the discovery phase, the triangular marker
            represent the time of link outage, and the diamond marker the time 
            of route change.
        }
    \label{fig:dyn_net_env}
\end{figure}

% section MEADcast in dynamic network environments (end)
Additionally, we observed instances where a router failed to perform \gls{m2u}
    transformation -- due to reasons such as an error, \gls{mead} being
    disabled, or a routing change -- \gls{mead} packets are forwarded to the
    client whose address is specified in the IP destination field.
% Furthermore we oberserved, that if the last router fails to execute the MEADcast to IP Unicast transformation
%     (due to reasons such as an error, MEADcast being disabled, or a routing
%     change), the MEADcast packets are forwarded to the client whose address is
%     indicated in the IP destination field.
This scenario potentially results in a leakage of the addresses of other
    group members.
However, in our experiments, we found that the client software (specifically
    \textit{``Iperf''} and \textit{``Netcat''}) disregarded the \gls{mead}
    header and processed the packet correctly.
Furthermore, by injecting discovery responses, we were able to impede
    \gls{mead} delivery to multiple receivers and intercept \gls{mead} packets. 

% section Results (end)

\section{Discussion} % (fold)
\label{sec:Discussion}

% Research Questions:

% Scenario identification
% - When to use MEADcast (Application Requirements, Network Environment)
% - No disadvantage in using MEADcast (bc of Fallback)
% - Limited network control, deploying a single router at key location can already signifiantly
%     improve performance (ref to r03)
% - 


% How does MEADcast perform compared to IP Unicast and IP Multicast?
% - throughput, latency, jitter, and resource utilization
% - Impact of discovery phase (Overhead, Shift from extensive unicast to MEADcast delivery)
% - Couldn't measure resource utilization but we argue less processing bc of
%   lower latency and drop rate for large number of receivers

% Which application and characteristics are well served by MEADcast?

% In which conditions is the usage of MEADcast sensible?
% - Prevailing circumstances, level of network control, endpoint distribution



% Performance very good, can reduce netload, bandwidth, time, latency
% Negligible effect of the discovery phase on bandwidth and latency.
%   We recommend relative low interval
% Splitting of packets after the first hop can be an issue
% Even in cases of higher endpoint distribution and one client per router
%   performance improvement. As long as a few links are shared
% Grouping gives the sender the ability to adjust the protocol to his needs.
    % large groups --> general higher bw savings, more clients effected by pkt loss
    % discovery interval --> often: latency spikes more often, faster recovery
    % merging nodes --> better performance for higher distribution of nodes
% Because of the resilience and adaptivity to changing 

This section evaluates the results from our experiments presented in
    \autoref{sec:Results} and aims to address the research questions
    formulated in \autoref{sec:Measurements}.
As outlined in \autoref{sec:Contribution}, the primary goal of this thesis is
    to conduct an evaluation of \gls{mead}, focusing on its 
    \textit{feasibility}, \textit{performance}, and \textit{potential
    application domains}.
\autoref{sub:discussion_Feasibility} discusses the results regarding
    feasibility, targeting \rqi{}.
\autoref{sub:discussion_Performance} elaborates on performance metrics
    in comparison to IP Unicast and IP Multicast, answering \rqii{}.
\autoref{sub:discussion_scenario} deduces use cases for which \gls{mead} is
    well-suited, addressing \rqiii{} and \rqiv{}.
Finally, \autoref{sub:MEADcast Revision Proposal} proposes several refinements
    to the \gls{mead} specification.

\subsection{Feasibility} % (fold)
\label{sub:discussion_Feasibility}
\begin{itemize}
\item[\textit{\rqi{}}]
    \textit{How robust is the current \gls{mead} specification? (deployment
        limitations \& structural issues of the protocol specification)}\par
    \paragraph{Deployment}
    The conducted series of use cases, encompassing deployment and evaluation,
        effectively demonstrates the feasibility of employing \gls{mead} within a
        medium-sized network.
    \gls{mead} deployment requires only the installation of sender and router
        software.
        Additionally, the fallback mechanism enables \gls{mead} to operate even
        without dedicated router support, presenting a distinct advantage over
        IP Multicast.
    In contrast to IP Multicast, which entails increased technical complexity
        and a compound routing procedure, necessitating all routers to support
        the protocol (see \autoref{sub:IP Multicast}), \gls{mead} imposes no
        additional requirements beyond the sender and router software.
    This characteristic facilitates a partial deployment of \gls{mead},
        underscoring its superior feasibility compared to IP Multicast.
    As we discussed below, this characteristic makes \gls{mead}
        particularly suitable in conditions of limited network control.

% How robust is the current specification?
% - deliberate routing changes
% - network disruption (link & router outage)
% - Anomaly handling
% - firewall (fallback mechanism)
% => adaptivity and suitability for dynamic network environments)
%%%%
% - Handles routing changes, and network disruption similar to Unicast.
%   Needs max. one discovery phase afterwards
% - Anomaly handling mostly implementation dependent (no router authorization so far)
% - Falling back to MEADcast if possible lead to way better results
    \paragraph{Robustness}
    \gls{mead} has demonstrated resilience and recovery from deliberate routing
        changes, network disruptions such as link and router outages, and
        packet discarding by intermediate nodes.
    As \gls{mead} operates based on IP Unicast routes, its adaptability to
        evolving network topologies primarily relies on the underlying routing
        protocol.
    However, if a packet's route is altered and it does not traverse the
        routers listed in its header, delivery disruption persists until the
        next discovery phase.
        In the event of a firewall dropping packets during \gls{mead} transmission,
        disruptions endure until the sender detects the modified topology tree
        in the subsequent discovery phase.
    Both \gls{mead} and IP Unicast fallback effectively address the presence of
        a firewall.
    However, reverting to a router positioned in front of the firewall results
        in 50\% less network bandwidth utilization compared to IP Unicast.

    \paragraph{Anomaly Handling}
    In cases where routers fail to perform \gls{m2u} transformation, packets
        are forwarded to the client specified in the IP destination field,
        potentially exposing group member IP addresses.
    \autoref{sub:MEADcast Revision Proposal} discusses a potential mitigation
        preventing the exposure of group information.
    It is important to note that the handling of anomalous discovery responses
        is implementation-specific.
    However, since no authentication mechanism is specified for \gls{mead},
        malicious discovery responses can be injected, potentially hindering
        transmission to multiple clients and intercepting \gls{mead} packets.
    This represents a security threat, as one of \gls{mead}'s objectives is the
        preservation of endpoint privacy.

    % \textbf{Packet replication:}
    \paragraph{Packet Replication} % (fold)
    \label{par:Discussion Packet Replication}
    The experiments have revealed an inefficiency within the \gls{mead} routing
        process.
    Specially, when multiple router addresses are included within a single
        packet, sharing a common path of \gls{mead} routers, the first \gls{mead}
        hop generates a replica for each router in the address list.
    This behavior, while technically correct, arises from the stateless nature
        of routers, which lack information regarding whether routers from the
        address list share another intermediate router that could instead
        perform the replica generation.
    Although the sender possesses knowledge of how long routers within a packet
        share the same path, the current \gls{mead} specification lacks a feature
        to determine the point of packet replication, thus preventing previous
        \gls{mead} routers from doing so.
    As depicted in \autoref{fig:link_bw_l2l3_100} this inefficiency leads to a
        significant increase in bandwidth utilization.
    In \autoref{sub:MEADcast Revision Proposal} we propose a feature, which
        enables the sender to control the point of packet replication.
    % paragraph Packet replication (end)

    These results emphasize the feasibility of employing \gls{mead} in
        medium-sized networks.
    Moreover, the experiments illustrate \gls{mead}'s resilience to network
        disruptions and recovery capabilities.
    This highlights the protocol's adaptivity and suitability for dynamic
        network environments, offering promising initial insights into the
        real-world applicability and resilience of \gls{mead}.
    However, anomaly handling is highly implementation-specific.
    Furthermore, in \autoref{sub:MEADcast Revision Proposal} we propose several
        refinements to the \gls{mead} specification and further investigation of
        their implications.
\end{itemize}
% subsesction Feasibility (end)

\subsection{Performance} % (fold)
\label{sub:discussion_Performance}
\begin{itemize}
\item[\textit{\rqii{}}]
    \textit{How does \gls{mead} perform compared to IP Unicast and IP Multicast?}
% Performance Evaluation:
% - Compare MEADcast with uni and multicast (efficiency and effectiveness)
%%%%%
    
% Resource utilization
%   - Sender load (See increase in transfer time UC1 4-5 clients for unicast)
%   - Latency and Drop rates grow 

    % NET BW, UP BW, TIME
    % Performance falls always between Unicast and Multicast
    % AVG Netload Reduction: Mead 56%, Multi 82.87% --> 32.42%
    % AVG Upstream Reduction: Mead 81.23%, Multi 96.15% --> 15.51%
    % AVG Transfer Time Reduction (UC2): Mead 49.18%, Multi 67.34% --> 26.97%
    The experiments emphasize that \gls{mead} significantly enhances performance
        metrics such as total network bandwidth utilization, sender upstream
        bandwidth utilization, and total transfer time compared to IP Unicast
        (see \autoref{sub:Results_Performance}).
    On average, \gls{mead} reduced network bandwidth utilization by more than
        half (56\%), sender upstream bandwidth utilization by 81.23\%, and
        total transfer time by half (\ucii{}: 49.18\%).
    In contrast, compared to IP Multicast, \gls{mead} increased network bandwidth
        utilization by 32.42\%, sender upstream bandwidth by 15.51\%, and total
        transfer time by 26.97\% (\ucii{}).

    % Implications of the discovery phase
    \autoref{sub:Results_Effects of the Discovery phase} illustrates that the discovery
        phase produces an average overhead of 4.69\% in total network bandwidth
        utilization, 18.36\% in sender upstream bandwidth utilization, and 0.88\%
        (\ucii{}) in total transfer time. %, and 39.57\% in average latency.
    However, excluding the initial discovery phase significantly reduces the 
        average overhead to an increase of 0.62\% in total network bandwidth
        utilization, and 0.42\% in sender upstream bandwidth utilization.
        %, and 15.69\% in average latency.

    % Resource utilization
    % We argue, that MEADcast results in a decrease in resouce utilization:
    % - for 4-5 clients Unicast latency strongly increases and packet loss starts
    %   to occurr.
    % - In contrast MEADcast latency is way lower and no or small packet loss
    % - Further we argue that the increase in latency for MEADcast with discovery
    %   supports that.
    % - bc for small numbers the diff in latency between with and without init
    %   discovery is ~11% and for high number, the overhead in latency is 75% with init discovery,
    %   showing Unicast processing utilizies the available resources.
    During most of our measurements, resource utilization peaked,
        rendering meaningful results for sender and router resource utilization
        unattainable.
    Nevertheless, we contend that \gls{mead} results in reduced resource
        utilization compared to IP Unicast as the number of receivers increases.
    % Although meaningful results for sender and router resource utilization were
    %     not attainable, we contend that \gls{mead} results in reduced resource
    %     utilization compared to IP Unicast as the number of receivers
    %     increases.
    \autoref{fig:latency_cmp} illustrates a distinct increase in average
        latency for IP Unicast transmissions with more than 42 group members
        (three clients per router), coinciding with the onset of packet loss.
    This suggests, that IP Unicast transmission exhausts the testbed's
        resources at this point.
    In contrast, \gls{mead} maintains constant latency levels with little to no
        packet loss, attributed to its lower resource utilization.
    Moreover, the observed increase in average latency for \gls{mead} with
        discovery phase supports our hypothesis.
    % As indicated in \autoref{tab:init_dcvr_latency}, the difference in latency
    %     increase with and without consideration of the initial discovery phase
    %     is only 11\% for three or fewer clients but rises to 75\% for more than
    %     three clients.
    As indicated in \autoref{tab:init_dcvr_latency}, recurring discovery phases
        entail a minor impact on average latency.
    In contrast, the initial discovery phase, accompanied by IP Unicast
        transmission significantly increases latency, network bandwidth utilization,
        sender upstream bandwidth consumption, and packet loss (see
        \autoref{sub:Results_Effects of the Discovery phase}).
    This suggests that IP Unicast transmission during the initial discovery
        phase is the primary cause of resource strain supporting our hypothesis.

    % Shift from extensive IP Unicast to MEADcast is possible,
    % leads to significant performance improvements, especially bw and latency reduction
    % results also in less resource utilization
    % Initial discovery causes major portion of the overhead produced by the
    % discovery mechanism, overhead of the recurrying disovery phase is negligible
    The results emphasize the feasibility of a graduate shift from extensive
        IP Unicast to \gls{mead} delivery, highlighting significant performance
        improvements, particularly in terms of reduced total network and sender
        upstream bandwidth utilization, as well as latency.
    Furthermore, the overhead generated by the discovery phase primarily
        originates from the initial discovery, whereas the overhead of
        subsequent recurring discovery phases is negligible.
% Effect of the discovery phase
\end{itemize}

% subsection Performance (end)

\subsection{Scenario identification} % (fold)
\label{sub:discussion_scenario}
\begin{itemize}
\item[\textit{\rqiii{}}]
    \textit{Which applications and characteristics are well served by \gls{mead}?}
    % File transfer
    % Stream
    % Discovery Phase can impede latency
    % Group:
    % - all tested group sizes from small (<10) up to <= 70 always beneficial
    % - the more members the better
    % - the longer the better, bc negligible overhead of subsequent discovery
    % Communication:
    % - recurring burst (e.g. File transfer) if long enough 
    % - steady stream (e.g. video/audio stream)
    % - not able to test asymmetric communication, but results from symmetric communication should be applicable
    % Network:
    % - No effect on Jitter measurable
    % - MEADcast could reduce latency, however may peak during discovery
    % - throughput intense is well-suited.

    Across all use cases, \gls{mead} has consistently demonstrated its
        benefits.

    % Group (Members, Distribution, Session length)
    From small group sizes ($<10$) to groups with $\leq 70$ receivers, \gls{mead}
        has shown improvements in all metrics.
    As the total number of group members and the number of receivers per
        router increased, these enhancements became more pronounced.
    Additionally, \gls{mead} exhibits performance improvements across all tested
        session durations.
    However, due to the overhead associated with the initial discovery phase,
        we recommend session durations of at least 5 seconds.
    Particularly with longer session durations, the performance improvements
        become more significant, as subsequent discovery phases represent a
        negligible overhead.
    % It is expected that there is an upper limit

    % Communication pattern (burst, steady, symmetric, asymmetric)
    \gls{mead} is well-suited for communication patterns characterized by steady
        flows (e.g. video/audio stream) and recurring bursts (e.g. file
        transfer).
    Although experiments with asymmetric communication patterns were not
        feasible due to resource limitations, we anticipate that the results
        from symmetric communication patterns are applicable, except for the
        enlarged traffic volume inherent in \gls{p2p} communication.

    % Network
    \gls{mead} proves beneficial across all group sizes, receiver distributions,
        session durations, and communication patterns.
    However, as group sizes, receiver clustering, and session duration
        increase, the performance improvements compared to IP Unicast become
        more pronounced.
    \gls{mead} excels in throughput-intense scenarios, with no discernible impact
        on jitter, making it suitable for applications sensitive to jitter
        fluctuations.
    However, \gls{mead} may not be suitable for applications sensitive to initial
        startup latency.

\item[\textit{\rqiv{}}]
    \textit{In which conditions is the usage of \gls{mead} sensible?}
    % limited resources
    % - especially upstream
    % limited network control
    % - deployment of router at key location can significantly reduce bw
    % offers sender the ability to shape how its packets should be transferred
    % - more MEADcast router does not mean better performance
    % - No disadvantage in deploying MEADcast, offers significant performance
    %   improvement already with a small number of receivers, and routers. And
    %   otherwise just falls back to Uni Cast
    % - If bandwidth should be reduced, but can not ensure Multicast support

    The employment of \gls{mead} proves particularly beneficial when the
        reduction of the total network bandwidth or sender upstream utilization
        is a major concern, especially in cases where IP Multicast support
        cannot be guaranteed.
    Deploying a single \gls{mead} router at a strategic location can
        significantly reduce bandwidth utilization between the sender and
        router.
    However, it is important to note that the presence of a higher number of
        \gls{mead} routers may not necessarily translate to performance
        enhancements, as indicated in \autoref{sub:discussion_Feasibility}
        highlighting packet replication inefficiencies.

    \gls{mead} excels in scenarios characterized by resource constraints and
        limited network control.
    Moreover, \gls{mead} empowers the sender to granularly tailor the traffic
        pattern according to specific requirements and prevailing network
        conditions.
    Given the protocol's support for partial deployment and resilient fallback
        mechanism, coupled with the absence of any major performance
        disadvantages, we advocate for the adoption of \gls{mead} whenever
        multicast communication is applicable.
    However, in scenarios where IP Multicast usage can be assured, it remains
        the preferred choice.
    % only disadvantage if MULTIcast works
    % to small can be prevented

\end{itemize}
% subsection Scenario identification (end)

\subsection{MEADcast Revision Proposal} % (fold)
\label{sub:MEADcast Revision Proposal}
% - Feasibility of deploying MEADcast (limitations & structural issues of specification)
%%%
% - Correct routing header
% - Omit Hop-by-Hop header
% - Each router interface needs an IP address
% - Packet gets split, which can lead to an increase in bandwidth utilization
% - If MEADcast router fails, IP addresses can be leaked
% - PLUS that no special routing is required (was hard to make Multicast routing)
Drawing from the experience gathered during the implementation and deployment
    of \gls{mead}, coupled with the findings of our experiment, we
    propose several refinements for the next \gls{mead} revision.

\paragraph{Protocol specification}
As discussed in \autoref{sec:Protocol Specification}, we propose the omission
    of the Hop-by-Hop IPv6 extension header, which experiences an increased
    drop rate \cite{rfc7872_ext_hdrs_drop_rate}.
This adjustment aims to reduce the protocol's overhead by eliminating a header
    that serves no purpose, decrease the likelihood of slow path processing,
    and increase the probability of being forwarded by non-\gls{mead} routers.
Furthermore, to align with RFC 8200 \cite{rfc8200_ipv6_hdr} and to mitigate the
    risk of intermediate nodes discarding \gls{mead} packets due to a malformed
    IPv6 routing extension, we advocate for the inclusion of the ``Segments
    Left'' field from the static IPv6 routing extension header, with a fixed
    value of zero, which prevents intermediate nodes that do not recognize
    the \gls{mead} header from discarding the packet \cite{rfc8200_ipv6_hdr}.
A detailed exposition of the proposed header specification is provided in
    \autoref{sec:Protocol Specification}.

\paragraph{Anomaly Handling} % (fold)
We recommend investigating whether targeting packets to the next \gls{mead}
    hop, rather than the first client from the address list, could prevent the
    exposure of group member information in cases of failure.
This investigation should also consider the implications in scenarios of route
    changes and network disruptions.
Since we were able to intercept traffic and impede data transmission to
    multiple clients, by injecting malicious discovery responses, we suggest
    the incorporation of an authentication mechanism for discovery responses
    similar to established routing protocols like OSPF \cite{rfc2328_ospf}.

\paragraph{Packet Replication} % (fold)
\label{par:Proposal Packet Replication}
As discussed in \autoref{par:Discussion Packet Replication}, our findings have
    revealed an inefficiency within the \gls{mead} routing process, if multiple
    router addresses within a single packet traverse a shared path of \gls{mead}
    routers.
To facilitate the sender to prevent the occurrence of premature packet
    replication, we propose the introduction of a ``Don't Replicate'' field in
    the \gls{mead} header.
Drawing inspiration from the IPv6 Hop Limit, this field contains a counter that
    decrements with each forwarding \gls{mead} router.
As long as the field retains value greater than zero, the packet should not be
    replicated, facilitating the sender to mitigate this inefficiency.
This feature can lead to significant bandwidth savings by determining the point
    of packet replication.
% designated router not stateless anymore
% no exposure of group member addresses

% paragraph Designated Router (end)

% paragraph Packet Replication (end)

% subsection MEADcast Revision Proposal (end)

% chapter Discussion (end)
% chapter Evaluation (end)
