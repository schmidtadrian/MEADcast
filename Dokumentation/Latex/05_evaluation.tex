\chapter{Evaluation} % (fold)
\label{chap:Evaluation}

\section{Results} % (fold)
\label{sec:Results}
This section presents the results of our experiments.
\autoref{sub:Results_Performance} provides a comparison of performance metrics among IP
    Unicast, MEADcast, and IP Multicast, including total network bandwidth
    utilization, sender upstream bandwidth utilization, and total transfer
    time.
\autoref{sub:Results_Grouping and Network support}, delves into the impact of different
    sender configurations and varying levels of MEADcast network support on
    these performance metrics as well as link bandwidth utilization, and
    latency.
In \autoref{sub:Results_Effects of the Discovery phase}, we examine the effect of the
    discovery phase on metrics such as bandwidth utilization, latency, and
    packet loss.
Lastly, \autoref{sub:Results_MEADcast in dynamic network environments} depicts the
    effect of events such as route change, link outages, and intermediate nodes
    dropping packets on MEADcast delivery.
\subsection{Performance Metrics} % (fold)
\label{sub:Results_Performance}
The experiments conducted consistently demonstrated that MEADcast outperformed
    Unicast.
Across all MEADcast configurations tested, there was a significant reduction in
    total network bandwidth utilization, sender upstream bandwidth utilization,
    and transmission time.

On average, the total network bandwidth was reduced by 50.29\% in EX1 Live
    stream (1 MB for 45s), 58.78\% in EX1 Live stream (2.5 MB for 45s), and
    58.93\% EX2 file transfer (25 MB) (see \autoref{tab:rel_save_net_bw}).
Moreover, in both EX1 and EX2, the savings increased by approximately 30\%
    when the number of clients per router increased from 1 to 5.
The relative reduction stayed constant across varying data volumes.
In comparison, IP Multicast achieved an average reduction of 80\%, with a
    similar improvement observed as the number of receivers increased.

\begin{table}
    \centering
        \begin{tabular}[c]{crrrrrr}
            \toprule
            \multirow{2}{*}[-2pt]{\makecell[c]{\textbf{Clients}\\\textit{(per Router)}}}
            & \multicolumn{2}{c}{\textbf{EX1} \textit{(1 MB/s)}}
            & \multicolumn{2}{c}{\textbf{EX1} \textit{(2.5 MB/s)}}
            & \multicolumn{2}{c}{\textbf{EX2}} \\
            \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
            &
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} & 
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} & 
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} \\
            \midrule
            1       & 25.50\%    & 59.59\%    & \makecell[c]{-}    & \makecell[c]{-}   & 42.11\%           & 64.69\%         \\
            2       & 49.20\%    & 79.79\%    & 50.13\%            & 79.76\%           & \makecell[c]{-}   & \makecell[c]{-} \\
            3       & 56.43\%    & 86.53\%    & 53.17\%            & 84.29\%           & 63.30\%           & 88.24\%         \\
            4       & 60.28\%    & 89.89\%    & 60.34\%            & 86.35\%           & \makecell[c]{-}   & \makecell[c]{-} \\
            5       & 60.06\%    & 91.45\%    & 71.49\%            & 88.62\%           & 71.38\%           & 94.24\%         \\
            \midrule
            Average & 50.29\%    & 81.45\%    & 58.78\%            & 84.76\%           & 58.93\%           & 82.39\%         \\
            \bottomrule
        \end{tabular}
    \caption[Total network bandwidth reduction]{Total network bandwidth reduction \textit{(relative to unicast)}}
    \label{tab:rel_save_net_bw}
\end{table}

Furthermore, we observed a substantial reduction of the sender upstream bandwidth
    utilization.
On average, it was reduced by 78.06\% in EX1 (1 MB/s for 45s), 81.03\% in EX1
    (2.5 MB/s), and 84.61\% in EX2 (25 MB) (see \autoref{tab:rel_save_upstream}).
Analog to the total network bandwidth reduction, the savings marginally
    increased with an increasing number of clients and remained consistent
    across various data volumes.
IP Multicast, however, achieved a remarkable relative reduction of 96.15\%.


\begin{table}
    \centering
        \begin{tabular}[c]{crrrrrr}
            \toprule
            \multirow{2}{*}[-2pt]{\makecell[c]{\textbf{Clients}\\\textit{(per Router)}}}
            & \multicolumn{2}{c}{\textbf{EX1} \textit{(1 MB/s)}}
            & \multicolumn{2}{c}{\textbf{EX1} \textit{(2.5 MB/s)}}
            & \multicolumn{2}{c}{\textbf{EX2}} \\
            \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
            &
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} & 
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} & 
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} \\
            \midrule
            1       & 64.61\%    & 92.90\%    & \makecell[c]{-}     & \makecell[c]{-}     & 79.84\%            & 89.03\%         \\
            2       & 78.12\%    & 96.47\%    & 78.08\%             & 96.45\%             & \makecell[c]{-}    & \makecell[c]{-} \\
            3       & 81.97\%    & 97.67\%    & 80.81\%             & 97.37\%             & 87.65\%            & 96.36\%         \\
            4       & 84.65\%    & 98.22\%    & 84.77\%             & 97.86\%             & \makecell[c]{-}    & \makecell[c]{-} \\
            5       & 80.95\%    & 98.31\%    & 80.45\%             & 97.64\%             & 86.33\%            & 97.86\%         \\
            \midrule
            Average & 78.06\% & 96.71\%   & 81.03\%             & 97.33\%             & 84.61\%            & 94.41\%         \\
            \bottomrule
        \end{tabular}
    \caption[Total sender upstream bandwidth reduction]{Total sender upstream bandwidth reduction \textit{(relative to unicast)}}
    \label{tab:rel_save_upstream}
\end{table}

In our experiments, MEADcast demonstrated notable reductions in the total
    transfer time.
On average MEADcast reduced the total transfer time by 8.94\% in EX1 (1 MB/s),
    8.93\% in EX1 (2.5 MB/s), and 24.16\% in EX2 (25 MB).
However, it is important to consider that the time savings observed in EX1 may
    be influenced by resource limitations of the testbed, potentially causing
    the fixed time period of 45 seconds to be exceeded during IP unicast
    transmission.
\autoref{fig:trans_time} illustrates a significantly lower growth rate of
    MEADcast compared to IP Unicast.
It rather scales more similar to IP Multicast in our experiments.
For instance, with 5 clients per router, Multicast reduced the transfer time
    by 77.97\%, from 2 minutes and 46 seconds to 36 seconds, while MEADcast 
    achieved a reduction of 57.46\%, resulting in a transfer time of 1 minute
    and 11 seconds \autoref{tab:rel_save_time}.
% section Performance (end)


% In file transfer experiment we verified, that transfering different sizes (25
%     \& 100 MB) results in the same relative savings.
% Therefore we tested with 25 MB. 

% General stats
% Impact Discovery
%   - Performs (netload, upstream, transmission time) only slightly (<5%) worse
%     than non discovery equivivalent
%   - Not noticiable in Jitter
%   - Noticiable in AVG Latency for higher number of clients (why?)
%   - Discovery Phase increases MAX latency (why?)
%   - No peaks in netload or upstream recognizable 
% Grouping
%   - Grouping does strongly influence performance
%   - The more addresses per packet the lower the upstream bandwidth
%   - (1) L2L3, (2) plain, (3) merge, L1
%   - In both experiments MEADcast support only on L2L3 results on certain
%     links in bandwidth savings of over 50%, because MEADcast router replicate
%     packets eventhough they travel the same path
% Dynamic network environments
%   - Turn on FW: 100% packet loss until next discovery phase (for us ca. 2s).
%     If falling back to a MEADcast router before the firewall, we experienced
%     an increase in netload bc of the earlier converstion from m2u.
%     If falling back to unicast way higher increase compared to falling back
%     to MEADcast. After allowing MEADcast traffic on FW again and next discovery
%     phase, back to normal.%
%   - Routing change: Not recognizable in RX BW and no packets lost. Higher latency.
%     Same behavior as Unicast.
%   - Link failure: RX BW drops to zero. After next discovery phase Link BW
%     increased bc of less optimal grouping. After routing change RX BW back up
% Always performed better than unicast and worse than multicast, no matter of
% config.
% We can see  that in tables ...

% Network bandwidth:
% - 56%
% - 82.72%
% Sender upstream:
% - 81.23%
% - 96.08%
% Transfer time:
% - 12.03% (24.16%)
% - 27.40% (49.18%)

% Conclusion:
% - less MEADcast support at the right spots performs better than 100%



\begin{table}
    \centering
        \begin{tabular}[c]{crrrrrr}
            \toprule
            \multirow{2}{*}[-2pt]{\makecell[c]{\textbf{Clients}\\\textit{(per Router)}}}
            & \multicolumn{2}{c}{\textbf{EX1} \textit{(1 MB/s)}}
            & \multicolumn{2}{c}{\textbf{EX1} \textit{(2.5 MB/s)}}
            & \multicolumn{2}{c}{\textbf{EX2}} \\
            \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
            &
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} & 
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} & 
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} \\
            \midrule
            1       & 0.65\%     & 0.66\%     & \makecell[c]{-}    & \makecell[c]{-}    & 37.94\%            & 61.54\%         \\
            2       & 1.11\%     & 1.10\%     & 2.93\%             & 2.90\%             & \makecell[c]{-}    & \makecell[c]{-} \\
            3       & 2.27\%     & 2.27\%     & 18.43\%            & 18.55\%            & 52.14\%            & 62.50\%         \\
            4       & 10.39\%    & 10.40\%    & 36.19\%            & 36.24\%            & \makecell[c]{-}    & \makecell[c]{-} \\
            5       & 30.30\%    & 30.22\%    & 38.82\%            & 38.94\%            & 57.46\%            & 77.97\%         \\
            \midrule
            % Average & 3.00\% &	8.94\%     & 8.93\%             & 24.09\%            & 24.16\%            &	49.18\% \\
            Average & 8.94\%     & 8.93\%     & 24.09\%            & 24.16\%            & 49.18\%            & 67.34\%         \\

            \bottomrule
        \end{tabular}
    \caption[Total transfer time reduction]{Total transfer time reduction \textit{(relative to unicast)}}
    \label{tab:rel_save_time}
\end{table}

\begin{figure}
    \begin{center}
        \input{Bilder/line_ex2_25mb_tx_time.pgf}
    \end{center}
    \caption[EX2 File transfer: Comparison of transfer time]{
        EX2 File transfer: Comparison of transfer time for a data volume of 25MB.
    }
    \label{fig:trans_time}
\end{figure}


% Fig X illustrates these metrics during two experiments.
% Network bandwidth mehr als halbiert (64%, 71%).
% Upstream um  (87%, 92%)
% Tansmit time (10%, 55%)
% Initial discovery klar zu erkennen
% EX1 stready, EX2 higher variation bc operating on limit
\autoref{fig:netload_upstream_cmp}, illustrates the performance enhancements in
    two specific scenarios.
In both cases, MEADcast substantially reduces network bandwidth utilization,
    achieving reductions of over half compared to IP unicast (EX1: 64\%, EX2:
    71\%).
Moreover, MEADcast achieves even greater reductions in sender upstream
    bandwidth utilization, reaching 87\% in EX1 and 92\% in EX2 compared to IP
    unicast.
At the start of the transmission, there is a discernible impact from the
    initial discovery phase.
During this phase, both network and upstream bandwidth experience peaks for
    approximately 2 seconds, reaching similar values as IP unicast, and even
    surpassing them in EX2.
The network and upstream bandwidth peaks for approximately 2 seconds to similar
    values as IP unicast, even exceeding it in EX2.
The bandwidth utilization in EX2 fluctuates more compared to EX1 because it
    pushes the testbed's resource usage to its limits.
MEADcast significantly reduces transmission time by 55\% compared to IP unicast.

\begin{figure}
    \begin{center}
        \input{Bilder/line_netload_upstream.pgf}
    \end{center}
    \caption[Comparison of network and sender upstream bandwidth utilization]{
        Comparison of network and sender upstream bandwidth utilization.
        On the left side, \textbf{(a)} depicts the total network bandwidth
        utilization and \textbf{(b)} the sender upstream bandwidth utilization
        during the execution of EX1 Live stream (1 MB/s for 45s) with 56 clients
        (5 per router), a maximum of 20 addresses per packet, and a discovery
        interval of 5 seconds.
        On the right side, \textbf{(c)} represents the total network bandwidth
        utilization and \textbf{(d)} the sender upstream bandwidth utilization
        during EX2 File transfer (25 MB) with 45 clients (5 per router), a
        maximum of 24 addresses per packet and a discovery interval of 5
        seconds.
    }
    \label{fig:netload_upstream_cmp}
\end{figure}



\subsection{Grouping and Network support} % (fold)
\label{sub:Results_Grouping and Network support}

We conducted tests on MEADcast using various groupings and different levels of
    MEADcast network support.
As previously noted, MEADcast consistently outperformed IP Unicast across all
    tested groupings and levels of network support.

Interestingly, we observed the most significant reduction in total network
    bandwidth utilization with MEADcast support on the Distribution (L2) and
    Access Layer (L3), as shown in \autoref{fig:netloadcmp}.
This was closely followed by 100\% network support and MEADcast with a merge 
    range of 2.
In contrast, the experiments with MEADcast support only on the Core Layer (L1)
    achieved the least improvement compared to IP unicast.
Each of these experiments were conducted with a maximum address list length of
    10 and 20.
Notably, in all tested scenarios in EX1, a maximum address list of 20
    consistently outperformed its counterpart with a limit of 10 addresses.
The drop in total network bandwidth utilization in EX1 (2.5 MB/s) is caused by
    an increased packet loss rate due to the resource limitations of the
    testbed.

\begin{figure}
    \begin{center}
        \input{Bilder/line_ex1_netload.pgf}
    \end{center}
    \caption[EX1 Live stream: Total network bandwidth utilization]{
        EX1 Live stream: Total network bandwidth utilization.}
    \label{fig:netloadcmp}
\end{figure}

\autoref{fig:upstreamcmp} illustrates, that all tested groupings and levels of
    network support exhibit significantly lower sender upstream bandwidth
    requirements compared to IP unicast.
Furthermore, as the number of receivers increases, the upstream bandwidth
    demands of MEADcast grow at a much slower rate compared to IP unicast.
Additionally, the higher the address list limit, the less packets with
    identical payload are required, thus the greater the reduction in sender
    upstream bandwidth utilization.
For instance, configurations with an address limit of 20 consistently
    outperformed all other configurations with a lower limit.
In contrast to the results of the total network bandwidth reduction,
    experiments with MEADcast support on L2 and L3 achieved the least
    improvements.
Experiments with 100\% MEADcast support and a merging range between 1 and 2
    yielded comparable results.

\begin{figure}
    \begin{center}
        \input{Bilder/line_upstream.pgf}
    \end{center}
    \caption[Total sender upstream bandwidth]{Total sender upstream bandwidth.}
    \label{fig:upstreamcmp}
\end{figure}

Continuing with the examination of the bandwidth savings resulting from
    MEADcast support solely on L2 and L3, \autoref{fig:link_bw_l2l3_100}
    highlights the bandwidth usage per link in experiments with 100\% MEADcast
    support versus protocol support exclusively on L2 and L3.
In both EX1 and EX2, the lower level of MEADcast support notably decreases
    the bandwidth usage on certain links by over 50\%.
    
In context of low levels of MEADcast support, we executed EX2 File transfer
    (25 MB), with 27 receivers, a maximum of 30 addresses er packet, and only
    $R_3$ supporting MEADcast.
This experiment lead to a reduction in transfer time by 42.98\%, network
    bandwidth utilization by 53.14\%, and sender upstream bandwidth utilization
    by 94.55\%.

\begin{figure}
    \begin{center}
        \input{Bilder/hbar_link_bw_l2l3_100.pgf}
    \end{center}
    \caption[EX2 File Transfer: Comparison of total link bandwidth]{
        EX2 File Transfer: Comparison of total link bandwidth.
        \textbf{(1)} Black bars denote results from experiments with 100\%
            MEADcast support.
        \textbf{(2)} Red bars denote results from experiments with MEADcast
            support on the Distribution (L2) and Access Layer (L3).
        \textbf{(a)} depicts the results from the execution of EX1 Live
            stream (1 MB/s for 45 seconds), with 70 receivers (5 per router),
            and a maximum of 20 addresses per packet.
        \textbf{(b)} depicts the results from the execution of EX2 File
            transfer (25 MB), with 45 receivers (5 per router), and a maximum
            of 24 addresses per packet.
    }
    \label{fig:link_bw_l2l3_100}
\end{figure}

%%%% LATENCY & JITTER %%%%
% Results:
%   Latency:
%    - Discovery Latency is similar or slightly higher than Uni
%    - AVG Latency: uni lower for small groups, switches with increasing group size
%    - MIN Latency: Uni best,  Mead worst
%    - MAX Latency: small group size all similar, increasing group size uni and mead discover worse
% Discussion:
%   Uni rises, bc we reach resource limits
%   Min uni best, bc without resource constraints its the fastest
%   --> MEADcast can save resources and therefore work in settings where Uni is not feasible
%   AVG latency for discovery rises mostly bc of uni cast usage during init discovery phase (see line latency)

The grouping configuration and the level of MEADcast network support also
    affects the latency.
For small groups MEADcast leads to an increase in average latency compared to
    IP unicast (see \autoref{fig:latency_cmp}).
As the number of receivers growth the latency of IP unicast significantly
    exceeds MEADcast's latency.
We measured the lowest latency with MEADcast support on L2 and L3 followed by
    100\% network support.
Analog to IP unicast, for large group sizes, MEADcast with periodic discovery
    phases yielded a higher average latency, however it is still lower than IP
    unicast.
The maximal latency shows similar results, besides that the latency of MEADcast
    with discovery phase is similar or slightly higher than IP unicast.

The minimal bandwidth of MEADcast is higher than IP unicast.
The protocol rather achieved similar results to IP Multicast.
All tested MEADcast configurations with a address list limit of 20 have a lower
    minimal latency than any configuration with a limit of 10.
The increase in latency, as the number of receivers growths, is the highest for
    the maximal latency and the lowest for the minimal latency.
During our experiments we also measured average, minimal and maximal Jitter.
However, the results indicate no measurable impact from MEADcast on Jitter.

\begin{figure}
    \begin{center}
        \input{Bilder/line_ex1_1mb_latency.pgf}
    \end{center}
    \caption[EX1 Live stream: Latency comparison]{
        EX1 Live stream: Latency comparison.
    }
    \label{fig:latency_cmp}
\end{figure}

% section Grouping and Network support (end)


\subsection{Effects of the discovery phase} % (fold)
\label{sub:Results_Effects of the Discovery phase}

% higher latency for a large number of receivers
% spikes in latency: for small number of receivers nearly no increase in avg latency
% for larger number of receivers notable peaks in both avg and max latency
As demonstrated previously, when dealing with a large number of receivers,
    MEADcast with periodic discovery phase tends to exhibit higher latency
    compared to MEADcast without discovery phase.

\autoref{fig:dcvr_latency_effect}, illustrates spikes in maximum latency for
    both 28 and 70 receivers during discovery phases.
Additionally, there is a noticeable increase in average latency for 70 clients
    during the discovery phase, although the increase is marginal for 28
    clients.
Especially the initial discovery phase results in a significant spike in
    total network bandwidth utilization, sender upstream bandwidth utilization,
    % and latency, as depicted in \autoref{fig:netload_upstream_cmp} and
    \autoref{fig:latency_cmp}).
Moreover, if the network is already operating at its capacity limit, the
    discovery phase can also result in increased packet loss, as shown in 
    \autoref{fig:dcvr_loss_effect}.
The initial discovery phase has also the most substantial effect on the packet
    loss.

Consequently, \autoref{tab:init_dcvr_latency} demonstrates the latency increase
    caused by the discovery phase, particularly focusing on the initial
    discovery.
In experiments with the discovery phase, the average latency increases by
    39.57\%.
Excluding the initial discovery phase and the associated IP Unicast
    transmission, the increase in latency is reduced by more than half to
    15.69\%.
Furthermore, the results emphasize a distinct increase in latency as the number
    of clients rises.
For $\leq 3$ clients per router, the latency increase is 11.1\% with the
    initial discovery and 3.9\% without it.
However, for $>3$ clients per router, the latency increase grows to 75.16\%
    with the initial discovery and to 30.83\% without it.

\autoref{tab:dcvr_net_up_effect} illustrates the impact of the discovery phase
    on network bandwidth utilization and sender upstream bandwidth utilization.
The discovery phase increases the network bandwidth utilization by an average
    of 4.69\%, while the sender upstream bandwidth utilization increases by
    18.36\% (compared to the corresponding experiment without discovery phase).
Excluding the initial discovery phase and the associated IP Unicast
    transmission, the increase in network bandwidth utilization is reduced to
    0.62\%, and for sender upstream utilization, it is reduced to 0.42\%
The theoretically calculated increase of the sender upstream bandwidth is
    merely 0.04\%.
The average increase in total transfer time is 0.88\% for EX2 File transfer
    (25 MB).

\begin{figure}
    \begin{center}
        \input{Bilder/line_latency_cmp.pgf}
    \end{center}
    \caption[Effect of the discovery phase on latency]{
        EX1 Live stream: Effect of the discovery phase on latency.
        The vertical dashed line represents the periodic discovery phase.
        The green line depicts the maximal latency and the blue line the
        average latency.
    }
    \label{fig:dcvr_latency_effect}
\end{figure}

\begin{figure}
    \begin{center}
        \input{Bilder/line_ex3_dcvr_loss.pgf}
    \end{center}
    \caption[Effect of the discovery phase]{
        Effect of the discovery phase.
        \textbf{(a)} depicts the received bandwidth on the sender.
        \textbf{(b)} depicts the packet loss on a receiver.
        \textbf{(c)} depicts the maximal latency on a receiver.
    }
    \label{fig:dcvr_loss_effect}
\end{figure}


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
\centering
\begin{tabular}{@{}ccrrrrrr@{}}
\toprule
    & & \multicolumn{4}{c}{\textbf{Average Latency} \textit{(ms)}} & \multicolumn{2}{c}{\textbf{Rel. increase} \textit{(\%)}} \\
    \cmidrule(lr){3-6}\cmidrule(lr){7-8}
        \makecell{\bfseries Clients\\\itshape (per Router)}
    &   \textbf{\makecell{Address\\limit}}
    &   \multicolumn{1}{c}{\makecell{No\\Dcvr}}
    &   \multicolumn{1}{c}{\makecell{Incl. init.\\Dcvr}}
    &   \multicolumn{1}{c}{\makecell{Excl. init.\\Dcvr}}
    &   \multicolumn{1}{c}{\makecell{Diff\\\itshape (\%)}}
    &   \multicolumn{1}{c}{\makecell{Incl. init.\\Dcvr}}
    &   \multicolumn{1}{c}{\makecell{Excl. init.\\Dcvr}} \\ \midrule
    1 & 10  &  2.77  & 2.72  & 2.69  & 0.86\%    & -2.17\%   & -3.05\%   \\
    2 & 10  &  3.14  & 3.81  & 3.54  & 6.93\%    & 17.51\%   & 11.37\%   \\
    3 & 10  &  4.31  & 4.51  & 4.20  & 7.01\%    & 4.40\%    & -2.81\%   \\
    4 & 10  &  5.15  & 24.16 & 6.42  & 73.42\%   & 78.70\%   & 19.86\%   \\
    5 & 10  &  12.42 & 59.28 & 33.98 & 42.67\%   & 79.05\%   & 63.46\%   \\ \midrule
    2 & 20  &  3.33  & 3.64  & 3.50  & 3.84\%    & 8.46\%    & 4.80\%    \\
    3 & 20  &  4.20  & 5.77  & 4.53  & 21.53\%   & 27.32\%   & 7.39\%    \\
    4 & 20  &  6.10  & 19.92 & 6.08  & 69.46\%   & 69.38\%   & -0.27\%   \\
    5 & 20  &  9.94  & 37.49 & 16.69 & 55.47\%   & 73.49\%   & 40.47\%   \\ \midrule
    & & & \multicolumn{2}{r}{\textbf{Average}}  & 31.24\%   & 39.57\%   & 15.69\% \\ \bottomrule
\end{tabular}
\caption[Latency increase caused by the discovery phase]{
    Latency increase caused by the discovery phase.
    We gathered the results by executing the experiments once with and once
        without periodic discovery phase.
    The relative difference describes the variation between the experiments
        conducted with and without the initial discovery phase.
    The relative increase refers to the corresponding experiment without
        discovery phase.
}
\label{tab:init_dcvr_latency}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
\centering
\begin{tabular}{@{}lrrrrrc@{}}
\toprule
    & & \multicolumn{5}{c}{\textbf{Bandwidth}} \\ \cmidrule(lr){3-7}
    & & \multicolumn{2}{c}{\textbf{\makecell{Inclusive initial\\discovery phase}}}
    & \multicolumn{3}{c}{\textbf{\makecell{Exclusive initial\\discovery phase}}} \\ \cmidrule(lr){3-4}\cmidrule(lr){5-7}
    \multicolumn{1}{c}{\textbf{Experiment}} &
    \multicolumn{1}{c}{\textbf{Time}} &
    \multicolumn{1}{c}{Network} &
    \multicolumn{1}{c}{Sender} &
    \multicolumn{1}{c}{Network} &
    \multicolumn{1}{c}{Sender} &
    \multicolumn{1}{c}{Sender \textit{(Calc.)}\tnote{1}} \\ \midrule
    EX1 \textit{(1 MB/s)}                   & 0.00\%  & 6.17\%  & 17.37\%   & 0.54\%    & -2.24\%   & 0.06\% \\
    EX1 \textit{(2.5 MB/s)}                 & -0.03\% & 2.79\%  & 16.35\%   & 0.78\%    & 2.48\%    & 0.03\% \\
    EX2 \textit{(25 MB)}                    & 0.88\%  & 5.11\%  & 21.37\%   & 0.55\%    & 1.01\%    & 0.02\% \\ \midrule
    \multicolumn{1}{c}{\textbf{Average}}    & 0.28\%  & 4.69\%  & 18.36\%   & 0.62\%    & 0.42\%    & 0.04\% \\ \bottomrule
\end{tabular}
    \caption[Bandwidth utilization increase caused by the discovery phase]{
        Bandwidth utilization increase caused by the discovery phase.
        We gathered the results by executing the experiments once with and once
            without periodic discovery phase.
        100\% referrs to the corresponding experiment without discovery phase.
    }
\label{tab:dcvr_net_up_effect}
\end{table}


Furthermore, we observed that in the current sender implementation, the order
    in which it receives discovery responses also affects the grouping of the
    clients.
However, we did not observe any performance implications of the change in
    receiver grouping.
% section Influence of the Discovery phase (end)

\subsection{MEADcast in dynamic network environments} % (fold)
\label{sub:Results_MEADcast in dynamic network environments}

% Fallback
% After enabeling FW, receive BW drops to 0, netload drops low
% After next discovery, client receive traffic again.
%   the net bw goes up (bc longer time or whole path uni)
% After disableing nothing happens in first place.
%   But after next discovery netload reduces, bc sender switches back to MEADcast
In EX3 P2P we evaluated MEADcasts resilience and recovery mechanisms required
    for dynamic network environments.

We deployed a firewall to intentionally drop MEADcast packets during the
    transmission.
\autoref{fig:dyn_net_env} (a) illustrates, that upon enabling the firewall,
    the receiver bandwidth drops to zero, and the network bandwidth utilization
    decreases as well.
After the next discovery phase, the sender tries to assign the clients to a
    router positioned in front of the firewall.
If this is not feasible, the sender falls back to IP Unicast transmission.
Consequently, the packets traverse the firewall and the client receives the 
    traffic again.
Fallback to IP unicast increases the total network bandwidth utilization from
    15 MB/s to 50 MB/s, whereas falling back to another MEADcast router raises
    the bandwidth only up to 28 MB/s.
When the firewall is disabled again, initially, no change is observed.
However, after the subsequent discovery phase, the sender switches back to the
    initial MEADcast grouping.
This results in the network bandwidth utilization returning to its previous
    value of 15 MB/s before the firewall was activated.

Furthermore, we assessed MEADcast's resilience to route changes.
In our experiments, we observed that the receiver bandwidth remained consistent
    despite altering a route during MEADcast transmission.
The only observed effect was a peak in latency.
After the route change, packet delivery remained undisrupted because we
    adjusted the traffic from $R_1$ to $R_3$ (\textit{r1\_r3}) to traverse via
    $R_4$ (\textit{r1\_r4}), ensuring the packet keeps traversing all MEADcast
    routers from its address list.
It is crucial to note that if a MEADcast packet does not pass through a router
    listed in its header, there is a risk of it not being processed correctly,
    potentially resulting in the packet not being delivered to its intended
    receivers. 


Lastly, we evaluated MEADcast's recovery from a link outage.
Immediately after the failure of the link between $R_1$ and $R_3$, the receiver
    bandwidth dropped to zero.
Following the subsequent discovery phase, the output bandwidth of $R_1$
    (\textit{r1\_r3}) increased from 1.5 MB/s to 8 MB/s.
This increase occurred due to a new grouping, characterized by an earlier
    MEADcast to IP Unicast transformation.
Once the underlying routing protocol detected the link failure and adjusted
    the route accordingly, the clients resumed receiving traffic.
Similar to the route change experiment, the routing protocol adjusted the
    traffic from $R_1$ to $R_3$ (\textit{r1\_r3}) to traverse via $R_4$
    (\textit{r1\_r4}), ensuring that the packet resumed traversing all
    MEADcast routers from its address list, resulting in correct packet
    processing and delivery.
After the next discovery phase, the sender reverted back to the initial
    grouping, lowering the link bandwidth utilization back to its previous
    value of 1.5 MB/s again.

\begin{figure}
    \begin{center}
        \input{Bilder/line_ex3_p2p_dyn_net_env.pgf}
    \end{center}
    \caption[MEADcast fallback and recovery mechanism]{
        EX3 P2P MEADcast fallback and recovery mechanism:
        \textbf{(1)} The left column illustrates the total network bandwidth
            \textbf{(a)} and receiver bandwidth \textbf{(b)} in a scenario
            where MEADcast packets are dropped during transmission, for
            instance, due to firewall intervention.
        The black line represents the results when falling back to IP Unicast
            transmission.
        The red line illustrates the results from scenarios where the clients
            could be assigned to a MEADcast router located in front of the
            firewall.
        The triangular marker indicate the period during which MEADcast packets
            are dropped and the square marker represent the discovery phase.
        \textbf{(2)} The middle column illustrates the bandwidth per link
            \textbf{(c)} and receiver bandwidth \textbf{(d)} in a scenario
            where a route change occurred during transmission.
        \textbf{(3)} The right column  shows the bandwidth per link
            \textbf{(e)} and receiver bandwidth \textbf{(f)} in a scenario
            where a link outage occurred during transmission.
        The square marker indicate the discovery phase, the triangular marker
            represent the time of link outage, and the diamond marker the time 
            of route change.
        }
    \label{fig:dyn_net_env}
\end{figure}

% section MEADcast in dynamic network environments (end)
Additionally, we observed that in cases where a router fails to perform the
    MEADcast to IP Unicast transformation -- due to reasons such as an error,
    MEADcast being disabled, or a routing change -- MEADcast packets are
    forwarded to the client whose address is specified in the IP destination
    field.
% Furthermore we oberserved, that if the last router fails to execute the MEADcast to IP unicast transformation
%     (due to reasons such as an error, MEADcast being disabled, or a routing
%     change), the MEADcast packets are forwarded to the client whose address is
%     indicated in the IP destination field.
This scenario potentially results in a leakage of the addresses of other
    group members.
However, in our experiments, we found that the client software (specifically
    Iperf and Netcat) disregarded the MEADcast header and processed the packet
    correctly.

% section Results (end)

\section{Discussion} % (fold)
\label{sec:Discussion}

% Research Questions:

% Scenario identification
% - When to use MEADcast (Application Requirements, Network Environment)
% - No disadvantage in using MEADcast (bc of Fallback)
% - Limited network control, deploying a single router at key location can already signifiantly
%     improve performance (ref to r03)
% - 


% How does MEADcast perform compared to IP-Unicast and IP-Multicast?
% - throughput, latency, jitter, and resource utilization
% - Impact of discovery phase (Overhead, Shift from extensive unicast to MEADcast delivery)
% - Couldn't measure resource utilization but we argue less processing bc of
%   lower latency and drop rate for large number of receivers

% Which application and characteristics are well served by MEADcast?

% In which conditions is the usage of MEADcast sensible?
% - Prevailing circumstances, level of network control, endpoint distribution



% Performance very good, can reduce netload, bandwidth, time, latency
% Negligible effect of the discovery phase on bandwidth and latency.
%   We recommend relative low interval
% Splitting of packets after the first hop can be an issue
% Even in cases of higher endpoint distribution and one client per router
%   performance improvement. As long as a few links are shared
% Grouping gives the sender the ability to adjust the protocol to his needs.
    % large groups --> general higher bw savings, more clients effected by pkt loss
    % discovery interval --> often: latency spikes more often, faster recovery
    % merging nodes --> better performance for higher distribution of nodes
% Because of the resilience and adaptivity to changing 

This chapter evaluates the results from our experiments presented in
    \autoref{chap:Evaluation} and aims to address the research questions
    formulated in \autoref{sec:Measurements}.
As outlined in \autoref{sec:Contribution}, the primary goal of this thesis is
    to conduct a real-world evaluation of MEADcast, focusing on the aspects of
    \textit{feasibility}, \textit{performance}, and \textit{scenario
    identification}.

\paragraph{Feasibility} % (fold)
\label{par:discussion_Feasibility}
\begin{itemize}
\item[\textit{RQ1}]
    \textit{How robust is the current MEADcast specification? (deployment
        limitations \& structural issues of the protocol specification)}\par
% - Feasibility of deploying MEADcast (limitations & structural issues of specification)
%%%
% - Correct routing header
% - Omit Hop-by-Hop header
% - Each router interface needs an IP address
% - Packet gets split, which can lead to an increase in bandwidth utilization
% - If MEADcast router fails, IP addresses can be leaked
% - PLUS that no special routing is required (was hard to make Multicast routing)
    \textbf{Protocol specification:}
    As discussed in \autoref{sec:Protocol Specification}, we propose the
        omission of the empty Hop-by-Hop IPv6 Extension, which experiences an
        increased drop rate \cite{rfc7872_ext_hdrs_drop_rate}.
    This adjustment aims to reduce the protocol's overhead by eliminating a
        header that serves no purpose, decrease the likelihood of slow path
        processing, and increase the probability of being forwarded by
        non-MEADcast routers.
    Furthermore, to align with RFC 8200 \cite{rfc8200_ipv6_hdr} and mitigate
        the risk of intermediate nodes dropping MEADcast packets due to a
        malformed IPv6 routing extension, we advocate for the inclusion of the
        ``Segments Left'' field from the static IPv6 routing header extension.

    \textbf{Deployment:}
    The conducted series of experiments, encompassing deployment and evaluation,
        effectively demonstrates the feasibility of employing MEADcast within a
        medium-sized network.
    MEADcast deployment requires only the installation of sender and router
        software.
    Additionally, the fallback mechanism enables MEADcast to operate even
        without dedicated router support, presenting a distinct advantage over
        IP Multicast.
    In contrast to IP Multicast, which entails increased technical complexity
        and a compound routing procedure, necessitating all routers to support
        the protocol (see \autoref{sub:IP Multicast}), MEADcast imposes no
        additional requirements beyond the sender and router software.
    This characteristic facilitates a partial deployment of MEADcast,
        underscoring its superior feasibility compared to IP Multicast.

% How robust is the current specification?
% - deliberate routing changes
% - network disruption (link & router outage)
% - Anomaly handling
% - firewall (fallback mechanism)
% => adaptivity and suitability for dynamic network environments)
%%%%
% - Handles routing changes, and network disruption similar to Unicast.
%   Needs max. one discovery phase afterwards
% - Anomaly handling mostly implementation dependent (no router authorization so far)
% - Falling back to MEADcast if possible lead to way better results
    \textbf{Robustness:}
    MEADcast has demonstrated resilience and recovery from deliberate routing
        changes, network disruptions such as link and router outages, and
        packet discarding by intermediate nodes.
    As MEADcast operates based on IP Unicast routes, its adaptability to
        evolving network topologies primarily relies on the underlying routing
        protocol.
    However, if a packet's route is altered and it does not traverse the
        routers listed in its header, delivery disruption persists until the
        next discovery phase.
    In the event of a firewall dropping packets during MEADcast transmission,
        disruptions endure until the sender detects the modified topology tree
        in the subsequent discovery phase.
    Both MEADcast and IP Unicast fallback effectively address the presence of
        a firewall.
    However, reverting to a router positioned in front of the firewall results
        in 50\% less network bandwidth utilization compared to IP Unicast.

    \textbf{Anomaly Handling:}
    In cases where a router fails to perform the MEADcast to IP Unicast
        transformation, packets are forwarded to the client specified in the IP
        destination field, potentially exposing group member IP addresses.
    We recommend investigating whether targeting packets to the next MEADcast
        hop, rather than the first client from the address list, can prevent
        the exposure of group member information in cases of failure.
    This investigation should also consider the implications in cases of route
        changes and network disruptions.
    It is important to note that the handling of anomalous discovery responses
        is implementation-specific.
    Since no authentication mechanism is specified for MEADcast, malicious
        discovery responses can be injected, potentially hindering transmission
        to multiple clients.

    \textbf{Packet replication:}
    The experiments have revealed an inefficiency within the MEADcast routing
        process.
    Specially, when multiple router addresses are included within a single
        packet, sharing a common path of MEADcast routers, the first MEADcast
        hop generates a replica for each router in the address list.
    This behavior, while technically correct, arises from the stateless nature
        of routers, which lack information regarding whether routers from the
        address list share another intermediate router that could instead
        perform the replica generation.
    Although the sender possesses knowledge of how long routers within a packet
        share the same path, the current MEADcast specification lacks a feature
        to determine the point of packet replication, thus preventing previous
        MEADcast routers from doing so.
    As depicted in \autoref{fig:link_bw_l2l3_100} this inefficiency leads to a
        significant increase in bandwidth utilization.
    To address this issue, we propose the introduction of a ``Don't Replicate''
        field in the header.
    Similar to the IPv6 Hop Limit, this field contains a counter that
        decrements with each forwarding MEADcast router.
    As long as the field is greater than 0, the packet should not be
        replicated, facilitating the sender to mitigate this inefficiency.

    These results emphasize the feasibility of employing MEADcast in
        medium-sized networks.
    Moreover, the experiments illustrate MEADcast's resilience to network
        disruptions and recovery capabilities.
    This highlights the protocol's adaptivity and suitability for dynamic
        network environments, offering promising initial insights into the
        real-world applicability and resilience of MEADcast.
    However, anomaly handling is highly implementation-specific.
    Furthermore, we propose several refinements to the MEADcast specification
        and further investigation of their implications.
\end{itemize}
% paragraph Feasibility (end)

\paragraph{Performance} % (fold)
\label{par:discussion_Performance}
\begin{itemize}
\item[\textit{RQ2}]
    \textit{How does MEADcast perform compared to IP-Unicast and IP-Multicast?}
% Performance Evaluation:
% - Compare MEADcast with uni and multicast (efficiency and effectiveness)
%%%%%
    
% Resource utilization
%   - Sender load (See increase in transfer time EX1 4-5 clients for unicast)
%   - Latency and Drop rates grow 

    % NET BW, UP BW, TIME
    % Performance falls always between Unicast and Multicast
    % AVG Netload Reduction: Mead 56%, Multi 82.87% --> 32.42%
    % AVG Upstream Reduction: Mead 81.23%, Multi 96.15% --> 15.51%
    % AVG Transfer Time Reduction (EX2): Mead 49.18%, Multi 67.34% --> 26.97%
    The experiments emphasize that MEADcast significantly enhances performance
        metrics such as total network bandwidth utilization, sender upstream
        bandwidth utilization, and total transfer time compared to IP Unicast.
    On average, MEADcast reduced network bandwidth utilization by more than
        half (56\%), sender upstream bandwidth utilization by 81.23\%, and
        total transfer time by half (EX2: 49.18\%).
    In contrast, compared to IP Multicast, MEADcast increased network bandwidth
        utilization by 32.42\%, sender upstream bandwidth by 15.51\%, and total
        transfer time by 26.97\% (EX2).

    % Implications of the discovery phase
    \autoref{sub:Results_Effects of the Discovery phase} illustrates that the discovery
        phase produces an average overhead of 4.69\% in total network bandwidth
        utilization, 18.36\% in sender upstream bandwidth utilization, 0.88\%
        (EX2) in total transfer time, and 39.57\% in average latency.
    However, excluding the initial discovery phase signififantly reduces the 
        average overhead to an increase of 0.62\% in total network bandwidth
        utilization, 0.42\% in sender upstream bandwidth utilization, and
        15.69\% in average latency.

    % Resource utilization
    % We argue, that MEADcast results in a decrease in resouce utilization:
    % - for 4-5 clients Unicast latency strongly increases and packet loss starts
    %   to occurr.
    % - In contrast MEADcast latency is way lower and no or small packet loss
    % - Further we argue that the increase in latency for MEADcast with discovery
    %   supports that.
    % - bc for small numbers the diff in latency between with and without init
    %   discovery is ~11% and for high number, the overhead in latency is 75% with init discovery,
    %   showing Unicast processing utilizies the available resources.
    Although meaningful results for sender and router resource utilization were
        not attainable, we contend that MEADcast results in reduced resource
        utilization compared to IP Unicast as the number of receivers
        increases.
    \autoref{fig:latency_cmp} illustrates a distinct increase in average
        latency for IP Unicast transmissions with more than three clients,
        coinciding with the onset of packet loss.
    This suggests, that IP Unicast transmission exhausts the testbed's
        resources at this point.
    In contrast, MEADcast maintains constant latency levels with little to no
        packet loss.
    Moreover, the observed increase in average latency for MEADcast with
        discovery phase supports our hypothesis.
    As indicated in \autoref{tab:init_dcvr_latency}, the difference in latency
        increase with and without consideration of the initial discovery phase
        is only 11\% for three or fewer clients but rises to 75\% for more than
        three clients.
    This suggests that IP Unicast transmission during the initial discovery
        phase is the primary cause for resource strain.

    % Shift from extensive IP Unicast to MEADcast is possible,
    % leads to significant performance improvements, especially bw and latency reduction
    % results also in less resource utilization
    % Initial discovery causes major portion of the overhead produced by the
    % discovery mechanism, overhead of the recurrying disovery phase is negligible
    The results emphasize the feasibility of a graduate shift from extensive
        IP Unicast to MEADcast delivery, highlighting significant performance
        improvements, particularly in terms of reduced total network and sender
        upstream bandwidth utilization, as well as latency.
    Furthermore, the overhead generated by the discovery phase is primarily
        originates from the initial discovery, whereas the overhead of
        subsequent recurring discovery phases is negligible.
% Effect of the discovery phase
\end{itemize}

% paragraph Performance (end)

\paragraph{Scenario identification} % (fold)
\label{par:discussion_scenario}
\begin{itemize}
\item[\textit{RQ3}]
    \textit{Which applications and characteristics are well served by MEADcast?}
    % File transfer
    % Stream
    % Discovery Phase can impede latency
    % Group:
    % - all tested group sizes from small (<10) up to <= 70 always beneficial
    % - the more members the better
    % - the longer the better, bc negligible overhead of subsequent discovery
    % Communication:
    % - recurring burst (e.g. File transfer) if long enough 
    % - steady stream (e.g. video/audio stream)
    % - not able to test asymmetric communication, but results from symmetric communication should be applicable
    % Network:
    % - No effect on Jitter measurable
    % - MEADcast could reduce latency, however may peak during discovery
    % - throughput intense is well-suited.

    Across all experiments, MEADcast has consistently demonstrated its
        benefits.

    % Group (Members, Distribution, Session length)
    From small group sizes ($<10$) to groups with $\leq 70$ receivers, MEADcast
        has shown improvements in all metrics.
    As the total number of group members and the number of receivers per
        router increased, these enhancements became more pronounced.
    Additionally, MEADcast exhibits performance improvements across all tested
        session durations.
    However, due to the overhead associated with the initial discovery phase,
        we recommend session durations of at least 5 seconds.
    Particularly with longer session durations, the performance improvements
        become more significant, as subsequent discovery phases represent a
        negligible overhead.
    % It is expected that there is an upper limit

    % Communication pattern (burst, steady, symmetric, asymmetric)
    MEADcast is well-suited for communication patterns characterized by steady
        flows (e.g. video/audio stream) and recurring bursts (e.g. file
        transfer).
    Although experiments with asymmetric communication patterns were not
        feasible due to resource limitations, we anticipate that the results
        from symmetric communication patterns are applicable, except for the
        enlarged traffic volume inherent in \gls{p2p} communication.

    % Network
    MEADcast proves beneficial across all group sizes, receiver distributions,
        session durations, and communication patterns.
    However, as group sizes, receiver clustering, and session duration
        increase, the performance improvements compared to IP Unicast become
        more pronounced.
    MEADcast excels in throughput-intense scenarios, with no discernible impact
        on jitter, making it suitable for applications sensitive to jitter.
    However, MEADcast may not be suitable for applications sensitive to initial
        startup latency.

\item[\textit{RQ4}]
    \textit{In which conditions is the usage of MEADcast sensible?}
    % limited resources
    % - especially upstream
    % limited network control
    % - deployment of router at key location can significantly reduce bw
    % offers sender the ability to shape how its packets should be transferred
    % - more MEADcast router does not mean better performance
    % - No disadvantage in deploying MEADcast, offers significant performance
    %   improvement already with a small number of receivers, and routers. And
    %   otherwise just falls back to Uni Cast
    % - If bandwidth should be reduced, but can not ensure Multicast support

    The employment of MEADcast proves particularly beneficial when the
        reduction of the total network bandwidth or sender upstream utilization
        is a major concern, especially in cases where IP Multicast support
        cannot be guaranteed.
    Deploying a single MEADcast router at a strategic location can
        significantly reduce bandwidth utilization between the sender and
        router.
    However, it is important to note that the presence of a higher number of
        MEADcast routers may not necessarily translate to performance
        enhancements, as indicated in \autoref{par:discussion_Feasibility}
        highlighting packet replication inefficiencies.

    MEADcast excels in scenarios characterized by resource constraints and
        limited network control.
    Moreover, MEADcast empowers the sender to granularly tailor the traffic
        pattern according to specific requirements and prevailing network
        conditions.
    Given the protocol's support for partial deployment and resilient fallback
        mechanism, coupled with the absence of any major performance
        disadvantages, we advocate for the adoption of MEADcast whenever
        multicast communication is applicable.
    However, in scenarios where IP Multicast usage can be assured, it remains
        the preferred choice.
    % only disadvantage if MULTIcast works
    % to small can be prevented

\end{itemize}
% paragraph  (end)


% chapter Discussion (end)
% chapter Evaluation (end)
