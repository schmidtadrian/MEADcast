\chapter{Results and Evaluation} % (fold)
\label{chap:Evaluation}
This chapter depicts the results of our experiments and elaborates on them.
\section{Performance} % (fold)
\label{sec:Performance}
The experiments conducted consistently demonstrated that MEADcast outperformed
    Unicast.
Across all MEADcast configurations tested, there was a significant reduction in
    total network bandwidth utilization, sender upstream bandwidth utilization,
    and transmission time.

On average, the total network bandwidth was reduced by 50.29\% in EX1 Live
    stream (1 MB for 45s), 58.78\% in EX1 Live stream (2.5 MB for 45s), and
    58.93\% EX2 file transfer (25 MB) (see \autoref{tab:rel_save_net_bw}).
Moreover, in both EX1 and EX2, the savings increased by approximately 30\%
    when the number of clients per router increased from 1 to 5.
The relative reduction stayed constant across varying data volumes.
In comparison, IP Multicast achieved an average reduction of 80\%, with a
    similar improvement observed as the number of receivers increased.

\begin{table}
    \centering
        \begin{tabular}[c]{crrrrrr}
            \toprule
            \multirow{2}{*}[-2pt]{\makecell[c]{\textbf{Clients}\\\textit{(per Router)}}}
            & \multicolumn{2}{c}{\textbf{EX1} \textit{(1 MB/s)}}
            & \multicolumn{2}{c}{\textbf{EX1} \textit{(2.5 MB/s)}}
            & \multicolumn{2}{c}{\textbf{EX2}} \\
            \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
            &
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} & 
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} & 
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} \\
            \midrule
            1       & 25.50\%    & 59.59\%    & \makecell[c]{-}    & \makecell[c]{-}   & 42.11\%           & 64.69\%         \\
            2       & 49.20\%    & 79.79\%    & 50.13\%            & 79.76\%           & \makecell[c]{-}   & \makecell[c]{-} \\
            3       & 56.43\%    & 86.53\%    & 53.17\%            & 84.29\%           & 63.30\%           & 88.24\%         \\
            4       & 60.28\%    & 89.89\%    & 60.34\%            & 86.35\%           & \makecell[c]{-}   & \makecell[c]{-} \\
            5       & 60.06\%    & 91.45\%    & 71.49\%            & 88.62\%           & 71.38\%           & 94.24\%         \\
            \midrule
            Average & 50.29\%    & 81.45\%    & 58.78\%            & 84.76\%           & 58.93\%           & 82.39\%         \\
            \bottomrule
        \end{tabular}
    \caption[Total network bandwidth reduction]{Total network bandwidth reduction \textit{(relative to unicast)}}
    \label{tab:rel_save_net_bw}
\end{table}

Furthermore, we observed a substantial reduction of the sender upstream bandwidth
    utilization.
On average, it was reduced by 78.06\% in EX1 (1 MB/s for 45s), 81.03\% in EX1
    (2.5 MB/s), and 84.61\% in EX2 (25 MB) (see \autoref{tab:rel_save_upstream}).
Analog to the total network bandwidth reduction, the savings marginally
    increased with an increasing number of clients and remained consistent
    across various data volumes.
IP Multicast, however, achieved a remarkable relative reduction of 96.15\%.


\begin{table}
    \centering
        \begin{tabular}[c]{crrrrrr}
            \toprule
            \multirow{2}{*}[-2pt]{\makecell[c]{\textbf{Clients}\\\textit{(per Router)}}}
            & \multicolumn{2}{c}{\textbf{EX1} \textit{(1 MB/s)}}
            & \multicolumn{2}{c}{\textbf{EX1} \textit{(2.5 MB/s)}}
            & \multicolumn{2}{c}{\textbf{EX2}} \\
            \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
            &
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} & 
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} & 
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} \\
            \midrule
            1       & 64.61\%    & 92.90\%    & \makecell[c]{-}     & \makecell[c]{-}     & 79.84\%            & 89.03\%         \\
            2       & 78.12\%    & 96.47\%    & 78.08\%             & 96.45\%             & \makecell[c]{-}    & \makecell[c]{-} \\
            3       & 81.97\%    & 97.67\%    & 80.81\%             & 97.37\%             & 87.65\%            & 96.36\%         \\
            4       & 84.65\%    & 98.22\%    & 84.77\%             & 97.86\%             & \makecell[c]{-}    & \makecell[c]{-} \\
            5       & 80.95\%    & 98.31\%    & 80.45\%             & 97.64\%             & 86.33\%            & 97.86\%         \\
            \midrule
            Average & 78.06\% & 96.71\%   & 81.03\%             & 97.33\%             & 84.61\%            & 94.41\%         \\
            \bottomrule
        \end{tabular}
    \caption[Total sender upstream bandwidth reduction]{Total sender upstream bandwidth reduction \textit{(relative to unicast)}}
    \label{tab:rel_save_upstream}
\end{table}

In our experiments, MEADcast demonstrated notable reductions in the total
    transfer time.
On average MEADcast reduced the total transfer time by 8.94\% in EX1 (1 MB/s),
    8.93\% in EX1 (2.5 MB/s), and 24.16\% in EX2 (25 MB).
However, it is important to consider that the time savings observed in EX1 may
    be influenced by resource limitations of the testbed, potentially causing
    the fixed time period of 45 seconds to be exceeded during IP unicast
    transmission.
\autoref{fig:trans_time} illustrates a significantly lower growth rate of
    MEADcast compared to IP Unicast.
It rather scales more similar to IP Multicast in our experiments.
For instance, with 5 clients per router, Multicast reduced the transfer time
    by 77.97\%, from 2 minutes and 46 seconds to 36 seconds, while MEADcast 
    achieved a reduction of 57.46\%, resulting in a transfer time of 1 minute
    and 11 seconds \autoref{tab:rel_save_time}.
% section Performance (end)


% In file transfer experiment we verified, that transfering different sizes (25
%     \& 100 MB) results in the same relative savings.
% Therefore we tested with 25 MB. 

% General stats
% Impact Discovery
%   - Performs (netload, upstream, transmission time) only slightly (<5%) worse
%     than non discovery equivivalent
%   - Not noticiable in Jitter
%   - Noticiable in AVG Latency for higher number of clients (why?)
%   - Discovery Phase increases MAX latency (why?)
%   - No peaks in netload or upstream recognizable 
% Grouping
%   - Grouping does strongly influence performance
%   - The more addresses per packet the lower the upstream bandwidth
%   - (1) L2L3, (2) plain, (3) merge, L1
%   - In both experiments MEADcast support only on L2L3 results on certain
%     links in bandwidth savings of over 50%, because MEADcast router replicate
%     packets eventhough they travel the same path
% Dynamic network environments
%   - Turn on FW: 100% packet loss until next discovery phase (for us ca. 2s).
%     If falling back to a MEADcast router before the firewall, we experienced
%     an increase in netload bc of the earlier converstion from m2u.
%     If falling back to unicast way higher increase compared to falling back
%     to MEADcast. After allowing MEADcast traffic on FW again and next discovery
%     phase, back to normal.%
%   - Routing change: Not recognizable in RX BW and no packets lost. Higher latency.
%     Same behavior as Unicast.
%   - Link failure: RX BW drops to zero. After next discovery phase Link BW
%     increased bc of less optimal grouping. After routing change RX BW back up
% Always performed better than unicast and worse than multicast, no matter of
% config.
% We can see  that in tables ...

% Network bandwidth:
% - 56%
% - 82.72%
% Sender upstream:
% - 81.23%
% - 96.08%
% Transfer time:
% - 12.03% (24.16%)
% - 27.40% (49.18%)

% Conclusion:
% - less MEADcast support at the right spots performs better than 100%



\begin{table}
    \centering
        \begin{tabular}[c]{crrrrrr}
            \toprule
            \multirow{2}{*}[-2pt]{\makecell[c]{\textbf{Clients}\\\textit{(per Router)}}}
            & \multicolumn{2}{c}{\textbf{EX1} \textit{(1 MB/s)}}
            & \multicolumn{2}{c}{\textbf{EX1} \textit{(2.5 MB/s)}}
            & \multicolumn{2}{c}{\textbf{EX2}} \\
            \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
            &
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} & 
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} & 
            \multicolumn{1}{c}{Mead} & 
            \multicolumn{1}{c}{Multi} \\
            \midrule
            1       & 0.65\%     & 0.66\%     & \makecell[c]{-}    & \makecell[c]{-}    & 37.94\%            & 61.54\%         \\
            2       & 1.11\%     & 1.10\%     & 2.93\%             & 2.90\%             & \makecell[c]{-}    & \makecell[c]{-} \\
            3       & 2.27\%     & 2.27\%     & 18.43\%            & 18.55\%            & 52.14\%            & 62.50\%         \\
            4       & 10.39\%    & 10.40\%    & 36.19\%            & 36.24\%            & \makecell[c]{-}    & \makecell[c]{-} \\
            5       & 30.30\%    & 30.22\%    & 38.82\%            & 38.94\%            & 57.46\%            & 77.97\%         \\
            \midrule
            % Average & 3.00\% &	8.94\%     & 8.93\%             & 24.09\%            & 24.16\%            &	49.18\% \\
            Average & 8.94\%     & 8.93\%     & 24.09\%            & 24.16\%            & 49.18\%            & 67.34\%         \\

            \bottomrule
        \end{tabular}
    \caption[Total transfer time reduction]{Total transfer time reduction \textit{(relative to unicast)}}
    \label{tab:rel_save_time}
\end{table}

\begin{figure}
    \begin{center}
        \input{Bilder/line_ex2_25mb_tx_time.pgf}
    \end{center}
    \caption[EX2 File transfer: Comparison of transfer time]{
        EX2 File transfer: Comparison of transfer time for a data volume of 25MB.
    }
    \label{fig:trans_time}
\end{figure}


% Fig X illustrates these metrics during two experiments.
% Network bandwidth mehr als halbiert (64%, 71%).
% Upstream um  (87%, 92%)
% Tansmit time (10%, 55%)
% Initial discovery klar zu erkennen
% EX1 stready, EX2 higher variation bc operating on limit
\autoref{fig:netload_upstream_cmp}, illustrates the performance enhancements in
    two specific scenarios.
In both cases, MEADcast substantially reduces network bandwidth utilization,
    achieving reductions of over half compared to IP unicast (EX1: 64\%, EX2:
    71\%).
Moreover, MEADcast achieves even greater reductions in sender upstream
    bandwidth utilization, reaching 87\% in EX1 and 92\% in EX2 compared to IP
    unicast.
At the start of the transmission, there is a discernible impact from the
    initial discovery phase.
During this phase, both network and upstream bandwidth experience peaks for
    approximately 2 seconds, reaching similar values as IP unicast, and even
    surpassing them in EX2.
The network and upstream bandwidth peaks for approximately 2 seconds to similar
    values as IP unicast, even exceeding it in EX2.
The bandwidth utilization in EX2 fluctuates more compared to EX1 because it
    pushes the testbed's resource usage to its limits.
MEADcast significantly reduces transmission time by 55\% compared to IP unicast.
\begin{figure}
    \begin{center}
        \input{Bilder/line_netload_upstream.pgf}
    \end{center}
    \label{fig:netload_upstream_cmp}
    \caption[Comparison of network and sender upstream bandwidth utilization]{
        Comparison of network and sender upstream bandwidth utilization.
        On the left side, \textbf{(a)} depicts the total network bandwidth
        utilization and \textbf{(b)} the sender upstream bandwidth utilization
        during the execution of EX1 Live stream (1 MB/s for 45s) with 56 clients
        (5 per router), a maximum of 20 addresses per packet, and a discovery
        interval of 5 seconds.
        On the right side, \textbf{(c)} represents the total network bandwidth
        utilization and \textbf{(d)} the sender upstream bandwidth utilization
        during EX2 File transfer (25 MB) with 45 clients (5 per router), a
        maximum of 24 addresses per packet and a discovery interval of 5
        seconds.
    }
\end{figure}



\section{Grouping and Network support} % (fold)
\label{sec:Grouping and Network support}

We conducted tests on MEADcast using various groupings and different levels of
    MEADcast network support.
As previously noted, MEADcast consistently outperformed IP Unicast across all
    tested groupings and levels of network support.

Interestingly, we observed the most significant reduction in total network
    bandwidth utilization with MEADcast support on the Distribution (L2) and
    Access Layer (L3), as shown in \autoref{fig:netloadcmp}.
This was closely followed by 100\% network support and MEADcast with a merge 
    range of 2.
In contrast, the experiments with MEADcast support only on the Core Layer (L1)
    achieved the least improvement compared to IP unicast.
Each of these experiments were conducted with a maximum address list length of
    10 and 20.
Notably, in all tested scenarios in EX1, a maximum address list of 20
    consistently outperformed its counterpart with a limit of 10 addresses.
The drop in total network bandwidth utilization in EX1 (2.5 MB/s) is caused by
    an increased packet loss rate due to the resource limitations of the
    testbed.

\begin{figure}
    \begin{center}
        \input{Bilder/line_ex1_netload.pgf}
    \end{center}
    \caption[EX1 Live stream: Total network bandwidth utilization]{
        EX1 Live stream: Total network bandwidth utilization.}
    \label{fig:netloadcmp}
\end{figure}

\autoref{fig:upstreamcmp} illustrates, that all tested groupings and levels of
    network support exhibit significantly lower sender upstream bandwidth
    requirements compared to IP unicast.
Furthermore, as the number of receivers increases, the upstream bandwidth
    demands of MEADcast grow at a much slower rate compared to IP unicast.
Additionally, the higher the address list limit, the less packets with
    identical payload are required, thus the greater the reduction in sender
    upstream bandwidth utilization.
For instance, configurations with an address limit of 20 consistently
    outperformed all other configurations with a lower limit.
In contrast to the results of the total network bandwidth reduction,
    experiments with MEADcast support on L2 and L3 achieved the least
    improvements.
Experiments with 100\% MEADcast support and a merging range between 1 and 2
    yielded comparable results.

\begin{figure}
    \begin{center}
        \input{Bilder/line_upstream.pgf}
    \end{center}
    \caption[Total sender upstream bandwidth]{Total sender upstream bandwidth.}
    \label{fig:upstreamcmp}
\end{figure}

Continuing with the examination of the bandwidth savings resulting from
    MEADcast support solely on L2 and L3, \autoref{fig:link_bw_l2l3_100}
    highlights the bandwidth usage per link in experiments with 100\% MEADcast
    support versus protocol support exclusively on L2 and L3.
In both EX1 and EX2, the lower level of MEADcast support notably decreases
    the bandwidth usage on certain links by over 50\%.
    
In context of low levels of MEADcast support, we executed EX2 File transfer
    (25 MB), with 27 receivers, a maximum of 30 addresses er packet, and only
    $R_3$ supporting MEADcast.
This experiment lead to a reduction in transfer time by 42.98\%, network
    bandwidth utilization by 53.14\%, and sender upstream bandwidth utilization
    by 94.55\%.

\begin{figure}
    \begin{center}
        \input{Bilder/hbar_link_bw_l2l3_100.pgf}
    \end{center}
    \caption[EX2 File Transfer: Comparison of total link bandwidth]{
        EX2 File Transfer: Comparison of total link bandwidth.
        \textbf{(1)} Black bars denote results from experiments with 100\%
            MEADcast support.
        \textbf{(2)} Red bars denote results from experiments with MEADcast
            support on the Distribution (L2) and Access Layer (L3).
        \textbf{(a)} depicts the results from the execution of EX1 Live
            stream (1 MB/s for 45 seconds), with 70 receivers (5 per router),
            and a maximum of 20 addresses per packet.
        \textbf{(b)} depicts the results from the execution of EX2 File
            transfer (25 MB), with 45 receivers (5 per router), and a maximum
            of 24 addresses per packet.
    }
    \label{fig:link_bw_l2l3_100}
\end{figure}

%%%% LATENCY & JITTER %%%%
% Results:
%   Latency:
%    - Discovery Latency is similar or slightly higher than Uni
%    - AVG Latency: uni lower for small groups, switches with increasing group size
%    - MIN Latency: Uni best,  Mead worst
%    - MAX Latency: small group size all similar, increasing group size uni and mead discover worse
% Discussion:
%   Uni rises, bc we reach resource limits
%   Min uni best, bc without resource constraints its the fastest
%   --> MEADcast can save resources and therefore work in settings where Uni is not feasible
%   AVG latency for discovery rises mostly bc of uni cast usage during init discovery phase (see line latency)

The grouping configuration and the level of MEADcast network support also
    affects the latency.
For small groups MEADcast leads to an increase in average latency compared to
    IP unicast (see \autoref{fig:latency_cmp}).
As the number of receivers growth the latency of IP unicast significantly
    exceeds MEADcast's latency.
We measured the lowest latency with MEADcast support on L2 and L3 followed by
    100\% network support.
Analog to IP unicast, for large group sizes, MEADcast with periodic discovery
    phases yielded a higher average latency, however it is still lower than IP
    unicast.
The maximal latency shows similar results, besides that the latency of MEADcast
    with discovery phase is similar or slightly higher than IP unicast.

The minimal bandwidth of MEADcast is higher than IP unicast.
The protocol rather achieved similar results to IP Multicast.
All tested MEADcast configurations with a address list limit of 20 have a lower
    minimal latency than any configuration with a limit of 10.
The increase in latency, as the number of receivers growths, is the highest for
    the maximal latency and the lowest for the minimal latency.
During our experiments we also measured average, minimal and maximal Jitter.
However, the results indicate no measurable impact from MEADcast on Jitter.

\begin{figure}
    \begin{center}
        \input{Bilder/line_ex1_1mb_latency.pgf}
    \end{center}
    \caption[EX1 Live stream: Latency comparison]{
        EX1 Live stream: Latency comparison.
    }
    \label{fig:latency_cmp}
\end{figure}

% section Grouping and Network support (end)


\section{Influence of the discovery phase} % (fold)
\label{sec:Influence of the Discovery phase}

% higher latency for a large number of receivers
% spikes in latency: for small number of receivers nearly no increase in avg latency
% for larger number of receivers notable peaks in both avg and max latency
As demonstrated previously, when dealing with a large number of receivers,
    MEADcast with periodic discovery phase tends to exhibit higher latency
    compared to MEADcast without discovery phase.

\autoref{fig:dcvr_latency_effect}, illustrates a spike in maximum latency for
    both 28 and 70 receivers during the discovery phase.
Additionally, there is a noticeable increase in average latency for 70 clients
    during the discovery phase, although the increase is marginal for 28
    clients.
Especially the initial discovery phase results in a significant spike in
    total network bandwidth utilization, sender upstream bandwidth utilization,
    and latency, as depicted in \autoref{fig:netload_upstream_cmp} and
    \autoref{fig:latency_cmp}).
Moreover, if the network is already operating at its capacity limit, the
    discovery phase can also result in increased packet loss, as shown in 
    \autoref{fig:dcvr_loss_effect}.
The initial discovery phase has also the most substantial effect on the packet
    loss.

\begin{figure}
    \begin{center}
        \input{Bilder/line_latency_cmp.pgf}
    \end{center}
    \caption[Effect of the discovery phase on latency]{
        EX1 Live stream: Effect of the discovery phase on latency.
        The vertical dashed line represents the periodic discovery phase.
        The green line depicts the maximal latency and the blue line the
        average latency.
    }
    \label{fig:dcvr_latency_effect}
\end{figure}

\begin{figure}
    \begin{center}
        \input{Bilder/line_ex3_dcvr_loss.pgf}
    \end{center}
    \caption[Effect of the discovery phase]{
        Effect of the discovery phase.
        \textbf{(a)} depicts the received bandwidth on the sender.
        \textbf{(b)} depicts the packet loss on a receiver.
        \textbf{(c)} depicts the maximal latency on a receiver.
    }
    \label{fig:dcvr_loss_effect}
\end{figure}

Furthermore we observed, that in the current sender implementation the order in
    which it receives discovery responses also effect the grouping of the
    clients.
However, we did not observed performance implication of the change in receiver
    grouping.
% section Influence of the Discovery phase (end)

\section{MEADcast in dynamic network environments} % (fold)
\label{sec:MEADcast in dynamic network environments}

% Fallback
% After enabeling FW, receive BW drops to 0, netload drops low
% After next discovery, client receive traffic again.
%   the net bw goes up (bc longer time or whole path uni)
% After disableing nothing happens in first place.
%   But after next discovery netload reduces, bc sender switches back to MEADcast
In EX3 P2P we tested MEADcasts capabilities in dynamic network environments.
We deployed a firewall between the sender and receivers to drop MEADcast
    packets.
During the transmission we enabled the firewall for 8 seconds.
\autoref{fig:dyn_net_env} illustrates, that after enabling the firewall the
    receiver bandwidth drops to zero and the network bandwidth utilization gets
    reduced as well.
After the next discovery phase the sender assigns the clients to a router in
    front of the firewall or falls back to unicast and the clients receive 
    traffic again.
In both cases the total network bandwidth goes up, however falling back to
    another MEADcast router saves nearly half the bandwidth.
When the firewall gets disabled, in the first place no change can be observed.
After the subsequent discovery phase, the sender switches back to MEADcast or 
    a router which is more closely located to the clients.

When a route gets changed during MEADcast transmission the receiver bandwidth
    remained constant.
We observed only an peak in latency.

\begin{figure}
    \begin{center}
        \input{Bilder/line_ex3_p2p_dyn_net_env.pgf}
    \end{center}
    \caption[MEADcast fallback and recovery mechanism]{
        EX3 P2P MEADcast fallback and recovery mechanism:
        \textbf{(1)} The left column illustrates the total network bandwidth
            \textbf{(a)} and receiver bandwidth \textbf{(b)} in a scenario
            where MEADcast packets are dropped during transmission, for
            instance, due to firewall intervention.
        The black line represents the results when falling back to IP Unicast
            transmission.
        The red line illustrates the results from scenarios where the clients
            could be assigned to a MEADcast router located in front of the
            firewall.
        The triangular marker indicate the period during which MEADcast packets
            are dropped and the square marker represent the discovery phase.
        \textbf{(2)} The middle column illustrates the bandwidth per link
            \textbf{(c)} and receiver bandwidth \textbf{(d)} in a scenario
            where a route change occurred during transmission.
        \textbf{(3)} The right column  shows the bandwidth per link
            \textbf{(e)} and receiver bandwidth \textbf{(f)} in a scenario
            where a link outage occurred during transmission.
        }
    \label{fig:dyn_net_env}
\end{figure}

% section MEADcast in dynamic network environments (end)
If the last router fails to execute the MEADcast to IP unicast transformation
    (due to reasons such as an error, MEADcast being disabled, or a routing
    change), the MEADcast packets are forwarded to the client whose address is
    indicated in the IP destination field.
This situation could potentially result in a leakage of the addresses of other
    group members.
In our experiments, the client software (Iperf and Netcat) ignored the MEADcast
    header and processed the packet correctly.

% chapter Evaluation (end)
