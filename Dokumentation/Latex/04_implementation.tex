\chapter{Implementation} % (fold)
\label{chap:Implementation}
This chapter introduces the employed protocol specification, the implementation
    of the MEADcast router and sender, as well as the testbed.

\section{Protocol Specification} % (fold)
\label{sec:Protocol Specification}
Since the existing literature on MEADcast lacks specific details regarding the
    header, such as the size of each field, we introduce the
    specifications utilized for our experiments.

Firstly, we omit the usage of an empty Hop-by-Hop IPv6 extension header as
    introduced by \citeauthor{meadcast1} \cite{meadcast1, meadcast2}.
We do this for several reasons:
First, to reduce the overhead of MEADcast.
Second, if a non-MEADcast router attempts to process Hop-by-Hop extension headers
    the packet probably end up in the router's slow path \cite{rfc7045}, potentially
    harming the protocol's performance.
Third, since the MEADcast header is intended to be processed exclusively by
    MEADcast routers, over which we maintain control, the Hop-by-Hop extension
    header serves no purpose.
Lastly, according to various research studies, packets containing a Hop-by-Hop
    extension header experience an increased drop rate of up to 40\% compared
    to packets without an extension header \cite{rfc7872_ext_hdrs_drop_rate},
    especially across multiple \glspl{as} \cite{rfc9098_ext_hdrs_op_impl, rfc9288}.

Next, we introduce the detailed characteristics of the MEADcast header in our
    implementation.
The first four octets of the MEADcast header constitute the static Routing
    extension header, which is shared by all routing variants.
This differs from previous implementations of MEADcast, which omitted the
    Segments Left field \cite{sdn_ba}.
We decided to include this field to comply with RFC 8200
    \cite{rfc8200_ipv6_hdr}, reducing the likelihood that intermediate nodes
    drop MEADcast packets due to a malformed Routing header.
Since there is no existing Routing Type designated for MEADcast, we utilize the
    experimental values of 253 and 254 in accordance with RFC3692
    \cite{rfc3692_ipv6_rt_type}.
The Segments Left field remains fixed to zero and is never altered because,
    according to RFC 8200 \cite{rfc8200_ipv6_hdr}, intermediate nodes that do
    not recognize the employed Routing type value must ignore the Routing
    header and proceed to process the next header.
An illustration of the MEADcast header layout can be found in
    \autoref{lst:meadcast_header}, along with an in-depth description of each
    field in \autoref{tab:meadcast_header}.
Due to the time frame of this thesis, we omit the usage of the optional port
    list as it does not affect the core MEADcast processing.

% No exact definition (size of each field, bit for bit)
% Removed IPv6 Hop-by-Hop header
%   - Reduce Size
%   - Often dropped (cite)
%   - If processed end up in slow path
%   - Only MEADcast router have to process the packet anyway
% Adjusted Routing Header
%   - Invalid routing header in BA (better use valid header)
%   - reduced size
%   - Omitted optional Port List for testing
% python protocol "Next Header:8,Hdr Ext Len:8,Routing Type:8,Num. Dst:8,Flags:2,Hops:6,Reserved:24,Delivery Map:32,Router Map:32,Address List:128"
% python protocol "Next Header:8,Hdr Ext Len:8,Routing Type:8,Segments Left:8,Num. Dst:8,Flags:2,Hops:6,Reserved:16,Delivery Map:32,Router Map:32,Address List:128"

% \begin{figure}[h!]
% \centering
% \begin{lstlisting}[xleftmargin=.07\textwidth]
%  0                   1                   2                   3
%  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
% +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
% |  Next Header  |  Hdr Ext Len  |  Routing Type |    Num. Dst   |
% +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
% |Fl.|    Hops   |                    Reserved                   |
% +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
% |                          Delivery Map                         |
% +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
% |                           Router Map                          |
% +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
% |                                                               |
% .                                                               .
% .                          Address List                         .
% .                                                               .
% |                                                               |
% +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
% \end{lstlisting}
% \caption{Python: MEADcast Header}
% \end{figure}

% \begin{figure}[!htbp]
% \centering
% \begin{lstlisting}[xleftmargin=.07\textwidth]
%  0                   1                   2                   3
%  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
% +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
% |  Next Header  |  Hdr Ext Len  |  Routing Type | Segments Left |
% +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
% |    Num. Dst   |Fl.|    Hops   |            Reserved           |
% +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
% |                          Delivery Map                         |
% +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
% |                           Router Map                          |
% +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
% |                                                               |
% .                                                               .
% .                          Address List                         .
% .                                                               .
% |                                                               |
% +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
%
% \end{lstlisting}
% \caption{MEADcast header specification}
% \label{lst:meadcast_header}
% \end{figure}

\begin{figure}[!htbp]
\centering
\begin{bytefield}[bitformatting=\tiny,bitwidth=1.1em,boxformatting=\centering\small]{32}
\bitheader{0-31} \\
    \begin{rightwordgroup}{\small RT Hdr \\ \small Preamble}
    \bitbox{8}{Next Header} &
    \bitbox{8}{Hdr Ext Len} &
    \bitbox{8}{Routing Type \\ \tiny (Experimental: 253/254)} &
    \bitbox{8}{Segments Left \\ \tiny (0)}
    \end{rightwordgroup} \\
    \bitbox{8}{Num. Dst} &
    % \bitbox{2}{Fl.} &
    \bitbox{1}{\tiny D\\[-1.25pt] C \\[-1.25pt] V} &
    \bitbox{1}{\tiny R\\[-1.25pt]S\\[-1.25pt]P} &
    \bitbox{6}{Hops} &
    \bitbox{16}{Reserved} \\
    \bitbox{32}{Delivery Bitmap} \\
    \bitbox{32}{Router Bitmap} \\
    \wordbox[lrt]{2}{Address List \\ \scriptsize (variable length)} \\
    \skippedwords \\
    \wordbox[lrb]{1}{} \\
\end{bytefield}
\caption{MEADcast header specification}
\label{lst:meadcast_header}
\end{figure}

\bgroup
\begin{table}[!htbp]
\centering
\def\arraystretch{1.35}%  1 is the default
\setlength{\tabcolsep}{1.2em}
\begin{tabularx}{\textwidth}{lX}
\toprule
\textbf{Field}& \textbf{Desciption} \\
\midrule
Next Header   & A 8-bit selector, identifying the type of the immediately
                following header \cite{rfc8200_ipv6_hdr}.
                Employs identical values as the IPv4 Protocol field (e.g. 17 =
                UDP, 59 = No Next Header for IPv6)
                \cite{iana_prot_nums}.\\
Hdr Ext Len   & An 8-bit unsigned integer representing the length of the
                Routing header in 8-octet units, excluding the initial 8
                octets \cite{rfc8200_ipv6_hdr}. \\
Routing Type  & An 8-bit identifier denoting a specific Routing header variant
                \cite{rfc8200_ipv6_hdr}.
                Given the absence of an existing Routing Type for MEADcast, we
                utilize the experimental values of 253 and 254 in accordance
                with RFC3692 \cite{rfc3692_ipv6_rt_type}.\\
Segments Left & An 8-bit unsigned integer representing the number of remaining
                route segments, indicating the number of explicitly listed
                intermediate nodes yet to be traversed before reaching the
                final destination \cite{rfc8200_ipv6_hdr}.
                Remains fixed to zero and is not modified by MEADcast routers.
                Consequently, intermediate nodes that do not recognize the
                employed Routing type value are required to disregard the
                MEADcast header and proceed to process the subsequent header
                \cite{rfc8200_ipv6_hdr}.\\
Num. Dst.     & An 8-bit unsigned integer denoting the number of IPv6
                addresses encoded in the address list.
                The field size sets a theoretical limit of 255 destinations.\\
% Flags         & A 2-bit Bitmap. The highest-order bit classifies the packet
%                 either as a data (0) or discovery  (1) packet.
%                 The lowest-order bit indicates whether the packet is a
%                 discovery request (0) or response (1) \cite{meadcast2}.
%                 Valid combinations comprise \inlinelst{00}, \inlinelst{10}, and
%                 \inlinelst{11}\\
\makecell[l]{Flags\\\hspace*{2mm} Dcv \\\hspace*{2mm} Rsp \\\ } &
\makecell[X]{A 2-bit Bitmap, valid combinations comprise \inlinelst{00},
                \inlinelst{10}, and \inlinelst{11}.\\
                1 bit classifying the packet as a data (0)
                or discovery (1) packet.\\
                1 bit identifying the packet as a discovery request (0) or
                response (1) \cite{meadcast2}.}\\
Hops          & An 6-bit unsigned integer, specifying the distance between the
                sender and a router, denoted in the number of MEADcast hops.
                This field is utilized during the discovery phase. The sender
                initializes the field with zero, and it is incremented by each
                intermediate MEADcast router encountered.
                The field size sets a theoretical limit of 63 hops.\\
Reserved      & A 16-bit reserved field.
                Should be initialized to zero during transmission and
                disregarded upon reception.\\
Delivery Map  & A 32-bit Bitmap, where each bit at position $i$ indicates
                whether a router at index $i$ in the address list has already
                been delivered (0 = delivered, 1 = not yet delivered)
                \cite{meadcast2}. \\
Router Map    & A 32-bit Bitmap, where each bit at position $i$ indicates
                whether an address at index $i$ in the address list is a
                router (0 = receiver, 1 = router) \cite{meadcast2}.\\
Address List  & A variable-length list comprising the IPv6 addresses of
                receivers and MEADcast routers.
                The field size of the Delivery and Router Bitmap imposes a
                practical limit of 32 addresses.\\
\bottomrule
\end{tabularx}
\caption{MEADcast header field description}
\label{tab:meadcast_header}
\end{table}
\egroup

% section Protocol Specification (end)

\section{Router} % (fold)
\label{sec:Router}
This section provides an overview of the MEADcast router implementation in the
    Linux Kernel.
The MEADcast routing software is developed and tested using the latest stable
    Kernel release, version 6.5.3, at the time of development.
MEADcast routing capabilities are tightly coupled to the flow of IPv6 packets
    through the Kernel network stack, making it infeasible to create a separate
    Kernel module loadable at runtime.
Therefore, we decided to extend the IPv6 module with MEADcast capability.
This feature can be enabled or disabled at compile-time using the
    \inlinelst{CONFIG\_IPV6\_MEADCAST} flag.
To avoid the burden of recompiling or switching the kernel to enable or disable
    MEADcast support, we introduce a runtime configuration flag.
This flag is managed through Sysctl, providing the virtual file
    \inlinelst{/proc/sys/net/ipv6/meadcast/enable}, which contains either 0 for
    disabled and 1 for enabled.
For instance, the root user can enable MEADcast support by running the command
    \inlinelst{"echo 1 > /proc/sys/net/ipv6/meadcast/enable"}.
However, it is crucial to ensure that this additional check does not adversely
    impact performance compared to a plain kernel, as it could potentially
    affect the reliability of experimental results.

As illustrated in \autoref{sub:Network Stack}, packets not designated to the
    router itself follow the ``forwarding'' packet flow in the Kernel.
IPv6 extension headers of these packets are normally ignored by the Kernel
    (except for the Router Alert extension).
Consequently, we embedded the entry point for MEADcast processing at the end of
    the \inlinelst{ip6\_forward} method located in
    \inlinelst{net/ipv6/ipv6\_output.c}.
If a packet contains the routing header extension the \inlinelst{mdc\_rcv}
    method located in \inlinelst{net/ipv6/mdcast.c} gets invoked.
This method performs basic sanity checks on the MEADcast header and then
    invokes \inlinelst{mdc\_dcv\_rcv} for discovery and
    \inlinelst{mdc\_data\_rcv} for data packets.

% discovery
% - validation
% - increment hop counter
% - clone socket buffer of incomming packet (discovery response)
% - update dst and src ip of discovery response
\textit{Discovery}: First, discovery packets get validated and the hop counter
    in the MEADcast header is incremented.
To create a discovery response the socket buffer struct of the incoming
    discovery request is cloned.
By this, we solely have to update the IP source and destination field of the
    cloned packet, set the response flag, and perform a routing table lookup
    for the sender's address.
The source IP address of the discovery response is set to the IP address
    associated with the interface returned from the routing lookup.
Lastly, the \inlinelst{dst\_output} method is invoked, dispatching the packet to
    the corresponding layer 2 handler.
The processing of the original packet does not need further handling, as it is
    already correctly done by the default forwarding flow.

\textit{Data}: Data packets undergo several steps upon arrival at the MEADcast
    router.
Firstly, they are validated, and the hop counter in the MEADcast header is
    incremented.
Subsequently, the router iterates over the Delivery and Router Bitmap to
    identify undelivered routers (indices set in both bitmaps).
During this iteration, if the router is not the last undelivered router in the
    sequence, the socket buffer of the original packet is cloned and forwarded
    accordingly.
However, if the router is the last undelivered router, the original packet can
    be forwarded without the need for costly replication.
To determine whether to clone the packet, the routers monitors whether an 
    undelivered router at index $r1$ has a successor (another undelivered
    router) at index $r2$, with $r1 < r2$.
The algorithm's specifics are depicted in \autoref{fig:router_hdr_parsing}.

For each undelivered router $R_u$ specified in the MEADcast header, the
    processing router $R_p$ decides whether to transmit the data via MEADcast
    or IP unicast.
If the IP address of $R_u$ in the MEADcast header belongs to the processing
    router $R_p$ itself, all designated endpoints will be delivered via IP
    unicast.
Additionally, if the number of endpoints assigned to the considered router
    $R_u$ from the MEADcast header is below a certain threshold, the processing
    router $R_p$ performs a premature MEADcast to IP unicast transformation.
For example, if $R_u$ has one designated endpoint, $R_p$ could execute a
    premature MEADcast to IP unicast transformation, potentially enhancing
    performance.
The threshold can also be configured during runtime by setting the parameter
    \inlinelst{/proc/sys/net/ipv6/meadcast/min\_dsts}.
Setting \inlinelst{min\_dsts} to 0 disables premature MEADcast to IP unicast
    transformation.

If the packet is forwarded via MEADcast, the router updates both the delivery
    bitmap and the destination field of the IPv6 header.
In the delivery bitmap, all bits are set to zero except for the index
    corresponding to the considered router.
Additionally, the IP destination field is set to the address of the first
    receiver following the router in the address list.
Lastly, the router performs a routing table lookup for the new destination
    address and forwards the packet accordingly.

In case of IP unicast transmission, the router converts the MEADcast packet
    into an IPv6 packet by copying the IPv6 header in front of the Layer 4
    header and adjusting the socket buffer pointers accordingly.
Additionally, the L4 checksum is corrected due to the modified destination IP
    address.

% Search for first router
%
% - validation
% - increment hop count
% - iterate bitmaps
%   - check which routers aren't delivered yet
%   - if router is not last undelivered router create a copy of socket buffer struct
%   - else use incoming one
% - forwarding
%   - if router addr in bitmap is of router or less than min_dsts eps -> m2u
%   - else: update bitmap (set all others to delivered), update ip dst field
% - m2u
%   - copy IP hdr in front of l3 header
%   - update skb pointers
%   - recalc checksum
\begin{figure}
    \begin{center}
        \includegraphics[scale=1]{router_hdr_parsing.pdf}
    \end{center}
    \caption{Router header parsing}
    \label{fig:router_hdr_parsing}
\end{figure}

\begin{figure}
    \begin{center}
        \includegraphics[width=\textwidth]{router_procedure.pdf}
    \end{center}
    \caption{MEADcast router procedure}
    \label{fig:router_procedure}
\end{figure}
% section Router (end)


\section{Sender} % (fold)
\label{sec:Sender}
This section provides an overview of our MEADcast sender implementation.
To ensure efficient processing that is comparable to existing network
    protocol implementations, we chose to implement the sender in the C
    language.
The main tasks of the sender include sending periodical discovery requests to
    all group members, receiving discovery responses, constructing a topology
    tree based on the received discovery responses, grouping receivers into
    MEADcast packets, and transmitting data to all group members either via
    MEADcast or IP unicast.

\paragraph{Startup} % (fold)
\label{par:Startup}
All receivers must be known at startup, dynamic runtime memberships are not
    supported.
We regard this decision as justifiable, given that MEADcast does not define any
    mechanism for joining or leaving a group, and the primary objective of this
    thesis is to compare the protocol with existing alternatives.
On startup the sender initializes a topology tree with itself as the root and
    all receivers as direct attached leafs.
Moreover, a TUN interface is created representing the common interface to other
    applications.
Any data, which is send to the TUN interface will be transmitted to the
    MEADcast group (see \autoref{fig:tun_dev}.
However, since MEADcast is a \textit{1:n} multicast protocol, the sender does not
    handle traffic directed from the MEADcast receivers towards the sender.
The \gls{mtu} of the TUN interface will be set to the MTU of the bind interface
    minus the maximum allowed MEADcast header size.
This ensures, that all packets read from the TUN interface can be transmitted
    via MEADcast.
Additionally, for convenience a host route to the TUN interface gets created 
    automatically (default: \inlinelst{fd15::1}).
Next, a discovery and transmission thread is created.
This is necessary to facilitate simultaneous data transmission and periodic
    MEADcast discovery.

\begin{figure}
    \begin{center}
        \includegraphics[width=\textwidth]{tun_dev.pdf}
    \end{center}
    \caption{Sender: TUN interface}
    \label{fig:tun_dev}
\end{figure}
% paragraph Startup (end)

\paragraph{TX thread} % (fold)
\label{par:TX thread}
The transmission thread operates on a straightforward pattern.
It reads data from the TUN device and transmits it to the MEADcast group either
    via MEADcast or IPv6 unicast.
The key element for data transmission is the \inlinelst{tx\_group} structure,
    which includes three members: \inlinelst{*mdc}, \inlinelst{nuni}, and
    \inlinelst{uni} (as detailed in \autoref{lst:txg_struct}).
These members encompass all the necessary information for data transmission,
    rendering \inlinelst{tx\_group} as the only data structure required by the
    transmission thread.
\inlinelst{*mdc} is a reference to a linked list of receivers grouped into
    MEADcast headers ready for transmission.
\inlinelst{nuni} contains the count of unicast addresses stored in
    \inlinelst{uni[]}.
Lastly, \inlinelst{uni[]} is an array of receivers served via IPv6 unicast.

The \inlinelst{tx\_group} structure is the only data accessed by both the
    discovery and transmisson thread.
Upon initialization of the transmission thread, a \inlinelst{tx\_group}
    structure is instantiated with all receivers added to the \inlinelst{uni[]}
    list and an empty \inlinelst{*mdc} list.
Subsequently, the \inlinelst{tx_group} is exclusively modified by the discovery
    thread.
To prevent race conditions on the \inlinelst{tx\_group}, the threads share an
    atomic reference pointing to the current group.

Next, the transmission thread enters an infinite loop executing the following
    steps.
Initially, it blockingly reads data from the TUN device.
Second, the sender atomically reads the current \inlinelst{tx\_group}.
This step is substantial because the \inlinelst{tx\_group} can be updated by
    the discovery thread at any time.
Finally, the data read from the TUN device is transmitted to the MEADcast group
    by first transmitting it via IPv6 unicast to all addresses listed in
    \inlinelst{uni[]} and subsequently transmitting it via MEADcast by copying
    the MEADcast headers stored in \inlinelst{*mdc} in front of the Layer 4
    header.
The copying of the MEADcast header overwrites the IPv6 header, necessitating
    the unicast transmission to occur before the MEADcast transmission.

\begin{listing}
\begin{minted}[linenos, frame=lines, framesep=2mm, breaklines]{c}
struct tx_group {
    struct child *mdc;
    size_t nuni;
    struct addr uni[];
};
\end{minted}
    \caption{Sender: tx\_group structure}
    \label{lst:txg_struct}
\end{listing}
% paragraph Tx thread (end)


\paragraph{Discovery Thread} % (fold)
\label{par:Discovery Thread}
% Discovery
% - Keep track of timers
% - Send and receive discovery
% - Perform grouping

% - init timer fd for discovery interval and discovery timeout
% - based on epoll (level-triggered)
% - enter infinitiv loop
%   - wait for an fd to become ready
%   - if interval:
%       - send discovery request to all group members
%       - disarms interval timer and enables timeout timer
%   - if mdc fd:
%       - receive discovery rsp, sanity checks
%       - modify topology tree (insert or update)
%   - if timeout:
%       - group receivers
%       - atomically update tx_group reference
%       - disarm timeout timer and enable interval timer
The discovery thread operates on a more intricate pattern compared to the
    transmission thread.
Its primary tasks include sending periodic discovery requests to all group
    members, receiving discovery responses from intermediate MEADcast routers,
    maintaining the topology tree, and grouping receivers into MEADcast
    headers.

To manage the recurring discovery phase, the sender utilizes a discovery
    interval and a discovery timeout timer (see
    \autoref{fig:discovery_timers}).
At the beginning of the discovery phase, the sender transmits discovery
    requests to all group members.
The discovery timeout determines how long the sender waits for discovery
    responses, while the discovery interval determines the duration between two
    consecutive discovery phases.
To monitor multiple timers, file descriptor-based timers are employed, which
    deliver timer expiration notifications via descriptors.
This allows for efficient monitoring using \inlinelst{epoll}
    \cite{man_timerfd}, an I/O event notification facility designed for
    monitoring multiple file descriptors \cite{man_epoll}.
% Besides the discovery interval and timeout file descriptors, we employ
%     \inlinelst{epoll} also to monitor the file descriptor used for transmitting
%     and receiving MEADcast.

On startup, the discovery thread initializes the timers and adds the file
    descriptors to \inlinelst{epoll}.
Besides the timers, \inlinelst{epoll} also monitors the MEADcast file
    descriptor.
Afterward, the discovery thread enters an infinite loop executing the following
    steps.
Initially, the thread invokes \inlinelst{epoll\_wait} (level-triggered), waiting
    for an event from one of the three file descriptors.
It either waits for one of the timers to expire or a MEADcast packet to arrive.

On expiration of the interval timer, the discovery thread sends discovery
    requests to all group members, disarms the interval timer, and starts the
    discovery timeout timer.

Upon receiving a MEADcast packet, the sender performs basic sanity checks and
    validation.
This includes verifying whether the address from the discovery response's
    address list is a MEADcast group member.
To facilitate, a fast lookup of IPv6 addresses a Judy array is utilized.
A Judy array is a sparse dynamic array with the key benefits of scalability,
    high performance, and memory efficiency \cite{web_judy}.
More precisely, we are using the \inlinelst{JudyHS} functions, being a
    hybrid of the best features of hashing  and  Judy  methods \cite{man_judy}.
\inlinelst{JudyHS} outperforms a hashing method across smaller and larger
    populations than the optimal hash table size, without necessitating any
    tuning or configuration \cite{man_judy}.
All receivers and routers are stored in the Judy array, using their IPv6
    address as the hashing key.
If a discovery response passes the checks and validation, the router
    is inserted into the topology tree or its existing entry is updated.
The position in the tree is determined by the distance (hops) in the discovery 
    response.

On expiration of the timeout timer, the sender starts grouping the receivers
    based on the topology tree.
A new transmission group (\inlinelst{tx\_new}) is created, and the atomic
    reference is updated to point to the newly created group (see:
    \autoref{fig:group_update}).
This procedure is required because complex structures can not be created
    atomically, prompting the decision to swing the atomic pointer after the
    new group's creation.


\begin{figure}
    \begin{center}
        \includegraphics[width=.95\textwidth]{discovery_timers.pdf}
    \end{center}
    \caption{Sender: Discovery Timers}
    \label{fig:discovery_timers}
\end{figure}

\begin{figure}
    \begin{center}
        \includegraphics[width=.9\textwidth]{group_update.pdf}
    \end{center}
    \caption{Sender: Atomic update of the transmission group}
    \label{fig:group_update}
\end{figure}
% paragraph Discovery Thread (end)

\paragraph{Grouping} % (fold)
\label{par:Grouping}
To understand the grouping of receivers into MEADcast packets, one has to
    understand the internal sender structures.

% basic grouping algorithm:
%   - search start (dfs)
%   - while free leaf:
%       - if cgs > max
%           - finish group
%           - goto search start
%       - if cgs >= max - 1
%           - add router to group
%           - recursively update parent counters
%           - finish group
%           - goto search start
%       - if cgs < max
%           - add router to group
%   - while free child:
%       - get free router child
%       - goto start
%   - if parent
%       - backpropagate
%       - if parent is root
%           - finish group
%       - goto start
    %
% effect of merge and split parameter

% hash table:
%   - stores pointer to node (router or leaf)
%   - used to quickly find ip
% *rcvr:
%   - linked list of all receivers
%   - used to quickly get all receivers
% tree:
%   - consists of nodes (router or leaf)
%   - sender is root
%   - used to group endpoints

% Grouping:
% 1) Reduce tree
%   - Remove routers with less leafs or routers than X. Childs are adopted by parent
% 2) Grouping
%   - search start (dfs)
% paragraph Grouping (end)

% Startup
% - Create TUN dev (set MTU, set Route)
% - txg only struct shared between threads
% - Implemented in C (efficiency & comparable to other protocols)
% - TUN
% - Automatically set route & MTU
% - Judy Array to quickly find IPs
% - Tree for grouping
% - grouping algorithm
% section Sender (end)

% \begin{algorithm}
% \caption{An algorithm with caption}
% \label{alg:two}
% \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
% \SetKw{KwGoTo}{go to}
%
% \Input{Router (s) to start grouping at}
% $n\gets 0$\;
% $m\gets 0$\;
% $r\gets getStart(s)$\;
% \While{r}{
% \texttt{start:}\\
%     \While{$r.freeLeafs < 0$}{
%         $m\gets n + r.freeLeafs + 1$\;
%         \uIf{$m>max$}{
%             \KwGoTo next\;
%         }
%         \uElseIf{$m\geq max-1}{
%             addRouterToGroup()\;
%             recursiveBackpropergate(r)\;
%             \KwGoTo next\;
%         }
%         \ElseIf{$m<max$}{
%             addRouterToGroup()\;
%             break\;
%         }
%     }
%
% \KwGoTo start\;
% }
% \end{algorithm}

\begin{algorithm}
    \caption{Sender: grouping algorithm}\label{alg:sender_grouping}
    \hspace*{\algorithmicindent} \textbf{Input:} Router ($s$) to start grouping at\\
    \hspace*{\algorithmicindent} \textbf{Output:} \texttt{tx\_group} structure\\
\begin{algorithmic}[1]
\State $group\gets NULL$\Comment{\texttt{tx\_group} structure}
\State $m\gets 0$
\State $n\gets 0$
\State $r\gets \text{getStart($s$)}$
\Comment{Gets next router based on depth-first search}
\While{$r$}
\Label \texttt{start:}
    \While{$r.freeLeafs > 0$}
        \State $m\gets n + r.freeLeafs + 1$
        \If{$m > max$}
            \State \textbf{go to} \texttt{start}
        \ElsIf{$m \geq max - 1$}
            \State addRouterToGroup($r$)
            \Comment{Update bitmaps and address list}
            \State recursiveBackpropergate($r$)
            \Comment{Update leaf and child count of all anchestors}
            \State \textbf{go to} \texttt{next}
        \ElsIf{$m < max$}
            \State addRouterToGroup($r$)
            \State \textbf{break}
        \EndIf
    \EndWhile
    \While{$r.freeChild > 0$}
        \State $c\gets \text{getFreeRouter($r$)}$
        \If{c}
            \State $r\gets \text{getRouter($c$)}$
            \State \textbf{go to} \texttt{start}
        \EndIf
    \EndWhile
    \If{$r.node.parent$}
        \State backPropagate($r$)
        \Comment{Update parent leaf and child count}
        \State $r\gets \text{getRouter($r.node.parent$)}$
        \If{\textbf{not}\ r.node.parent}
            \Comment{If parent is root finish group}
            \State \textbf{go to} \texttt{next}
        \EndIf

        \State \textbf{go to} \texttt{start}
    \EndIf
\Label \texttt{next:}
    \State finishGroup()
    \Comment{Adds MEADcast header to \texttt{group.mdc} list}
    \State $r\gets \text{getStart($r$)}$
\EndWhile
    \State addRemainingLeafs($s$)
    \Comment Adds the remaining leafs to \texttt{group.uni} list 
    \State \Return $group$
\end{algorithmic}
\end{algorithm}


\section{Experiment} % (fold)
\label{sec:Experiment}

\subsection{Testbed} % (fold)
\label{sub:Testbed_Implementation}
% - python script reading json
% - kvm, direct kernel boot, debian nocloud image
% - chrony time sync (had to change boot param on host system)
% - how measured

\paragraph{EX1: Live Stream} % (fold)
\label{par:impl EX1: Live Stream}

\begin{figure}[!htbp]
    \begin{center}
        \includegraphics[width=.75\textwidth]{exp_live_stream.pdf}
    \end{center}
    \caption{Experiment: Live stream (\textit{sender is red})}
    \label{fig:exp_live_stream}
\end{figure}

\begin{itemize}\itemsep0em
    \item \textbf{Goal:} Measure \textit{1:n} MEADcast performance for a
        mid to large-sized, comprising a mixture of highly clustered and spread
        members, highlighting the protocols potential for heterogen receiver
        distribution.
    \item[] \textbf{Characteristics}
    \begin{itemize}\itemsep0em
        \item Use case: streaming a campus or business event across multiple
            locations/buildings (requires multiple network domains)
        \item Requires both local clustering and spread, to depict typical
            campus layout (e.g. library high clustering, low receiver density
            for remote offices)
        \item Requires varying distance of receivers
        \item 26 routers, $14-70$ receivers
        \item Recurring downstream peaks to buffer content
            \cite{cartesian_us_bw}
    \end{itemize}
    \item[] \textbf{Expectations}
    \begin{itemize}\itemsep0em
        \item Strongly differing levels of bandwidth reduction for highly
            clustered and widely distributed endpoints
        \item E.g. Merging receivers under $R_{41}$ can significantly reduce
            the utilized bandwidth 
        \item E.g. Receivers in network $C_{28}$ can be used to measure
            the effect of MEADcast processing on latency and jitter compared to
            unicast delivery
    \end{itemize}
    \item[] \textbf{Testing Parameters}
    \begin{itemize}\itemsep0em
            \item Network: Degree of MEADcast support (e.g. 25\%, 50\%, \dots,
                dist. layer only)
        \item Sender: Max group size, merging range, splitting siblings
        \item Router: Premature M2U
    \end{itemize}
\end{itemize}
% paragraph EX1: Live Stream (end)
\newpage


\paragraph{EX2: File transfer} % (fold)
\label{par:implEX2: File transfer}

\begin{figure}[!htbp]
    \begin{center}
        \includegraphics[width=.75\textwidth]{exp_file_transfer.pdf}
    \end{center}
    \caption{Experiment: File transfer (\textit{sender is red})}
    \label{fig:exp_file_transfer}
\end{figure}

\begin{itemize}\itemsep0em
    \item \textbf{Goal:} Measure \textit{1:n} MEADcast performance for a
        mid to large-sized, highly clustered group, highlighting the protocols
        potential under ideal conditions.
    \item[] \textbf{Characteristics}
    \begin{itemize}\itemsep0em
        \item Use case: Rollout of software update (e.g. server OS)
        \item Requires high clustering since servers are usually located within
            one network domain
        \item 13 Router, $9-45$ Receivers
        \item Max out all the available bandwidth for a short period of time
            \cite{cartesian_us_bw}
    \end{itemize}
    \item[] \textbf{Expectations}
    \begin{itemize}\itemsep0em
        \item For a small number of receivers per router, minor reduction in
            bandwidth is expected compared to unicast (bc the first MEADcast
            router will split packets)
        \item For bigger numbers of receivers per router, more significant
            saving in bandwidth is expected
        \item High bandwidth savings on sender link across all scenarios
    \end{itemize}
    \item[] \textbf{Testing Parameters}
    \begin{itemize}\itemsep0em
        \item similar to \autoref{par:impl EX1: Live Stream}
    \end{itemize}
\end{itemize}
% paragraph EX2: File transfer (end)
\newpage

\paragraph{EX3 Video conference} % (fold)
\label{par:impl EX3 Video conference}

\begin{figure}[!htbp]
    \begin{center}
        \includegraphics[width=.75\textwidth]{exp_video_conference.pdf}
    \end{center}
    \caption{Experiment: P2P Video Conference
        (\textit{all members are senders})}
    \label{fig:exp_video_conference}
\end{figure}

\begin{itemize}\itemsep0em
    \item \textbf{Goal:} Measure \textit{m:n} MEADcast performance for a
        small to mid-sized group, comprising a mixture of highly clustered and
        spread members, highlighting the protocols potential for heterogen
        receiver distribution.
    \item[] \textbf{Characteristics}
    \begin{itemize}\itemsep0em
        \item Video conference by a department or project team
        \item Requires P2P connection and a steady stream of up and download
            \cite{cartesian_us_bw}
        \item Requires both local clustering and spread, to depict typical work
            culture (e.g. high clustering in office, and a few members like
            a customer or a colleague in home office being further away)
        \item Requires varying distance of group members
        \item 18 routers, $12-60$ senders/receivers
    \end{itemize}
    \item[] \textbf{Expectations}
    \begin{itemize}\itemsep0em
        \item Sheds light on, whether MEADcast is capable of bridging the
            asymmetric access link \cite{xcast_rfc, cartesian_us_bw}
        \item Demonstrates how expensive MEADcast processing is in comparison
            to unicast
        \item Highlights the differences in measurements between highly
            clustered and wider distributed areas
    \end{itemize}
    \item[] \textbf{Testing Parameters}
    \begin{itemize}\itemsep0em
        \item similar to \autoref{par:impl EX1: Live Stream}
    \end{itemize}
\end{itemize}
% paragraph EX3 Video conference (end)
\newpage

\paragraph{EX4 Online Gaming} % (fold)
\label{par:EX4 Online Gaming}

\begin{figure}[!htbp]
    \begin{center}
        \includegraphics[width=.75\textwidth]{exp_online_gaming.pdf}
    \end{center}
    \caption{Experiment: Online gaming}
    \label{fig:exp_online_gaming}
\end{figure}

\begin{itemize}\itemsep0em
    \item \textbf{Goal:} Measure MEADcast performance in disadvantageous
        setting in terms of distribution.
        Highlight the threshold at which the number of receivers per router
        makes MEADcast favourable. 
    \item[] \textbf{Characteristics}
    \begin{itemize}\itemsep0em
        \item Online multiplayer game with a high receiver distribution
        \item Frequent, downstream peaks in intervals of $2-4$ seconds
            \cite{cartesian_us_bw}
        \item 27 routers, $9-45$ receivers
        \item Requires long distances, to measure whether MEADcast processing
            significantly impacts latency and jitter.
    \end{itemize}
    \item[] \textbf{Expectations}
    \begin{itemize}\itemsep0em
        \item Demonstrates at which number of receivers per router, MEADcast
            shows advantageous performance (e.g. bandwidth).
        \item Demonstrates how expensive MEADcast processing is compared to
            unicast
    \end{itemize}
    \item[] \textbf{Testing Parameters}
    \begin{itemize}\itemsep0em
        \item similar to \autoref{par:impl EX1: Live Stream}
    \end{itemize}
\end{itemize}
% paragraph EX4 Online Gaming (end)

% subsection Testbed (end)


\subsection{Technical Infrastructure} % (fold)
\label{sub:Technical Infrastructure}
%%% Facts
% Hardware Resources
% Debian Version
% KVM
% para virtualized Net IO Dev
% One backing file, and vm disk only tracking differences.
% Debian No-Cloud Image Version for clients
% Direct Kernel boot for router
% VM resources
Given the impracticality of creating a physical testbed, we have opted for a 
    virtual environment using \gls{kvm}.
The testbed comprises \glspl{vm} connected by virtual networks.
The hypervisor, based on Debian 11.3, is equipped with 4 CPUs, 8 threads, 32 GB
    memory, and 120 GB of storage capacity.

Each VM is configured with 1 vCPU, 256 MB, and 8 GB of storage.
To optimize resource utilization, we employ the para-virtualized ``virtio''
    network adapter for the \glspl{vm}.
We utilize two types of \glspl{vm}: clients and routers, both based on a Debian
    11 no-cloud image.
No-cloud images streamline the setup process by bypassing the need for \gls{os}
    installation.
Additionally, direct Kernel boot is facilitated for the router, enabling swift
    kernel exchange by simply replacing the Kernel image file on the
    hypervisor.

% template images
During the build process, a router and a client template image are created.
These serve as \textit{``Qemu''} backing files for the \gls{vm} images,
    ensuring that \glspl{vm} only have to monitor changes compared to the
    backing template, reducing storage usage and build time significantly.

%%% How created
% python script virsh util
% from json file describing topo
% frr for routing, had to change repo bc ospf6 was missing
% ospf6 and Pim as routing protocols
% ospf domains
% chrony ptp for time sync, had to add kernel boot arg to make it work
% each vm a IPv6 only MGM if and other interfaces IPv6 only
% IP addressing concept
% each transit link neads a non link local address to make MEADcast work
% command to build topo
Automating the build process is a Python script provided in the digital
    appendix, with the command to construct our topology shown in
    \autoref{lst:cmd_mk_topo_start}.
The script presents a wrapper around tools such as ``\textit{virsh}'',
    ``\textit{libguestfs-tools}'', and ``\textit{qemu-img}''.
It accepts several input parameters, including template configuration files
    for the virtual network generation, ``\textit{chrony}'' (time
    synchronization), and ``\textit{frr}'' (routing).
Additionally, the script requires a Kernel image for the router and a
    configuration file describing the network topology.
\autoref{lst:topo_cfg_format} demonstrates an exemplary topology definition,
    resulting in the topology shown in \autoref{fig:ex_topo}.
The  \texttt{"stub"} key from the topology configuration contains a list of
    objects describing a stub network, its number of clients, the upstream
    router, and the OSPF area.
The \texttt{"trans"} key includes a list of objects comprising a tuple of two
    router IDs and OSPF area.
The routers are interconnected according to this list.
The \texttt{"area\_boundary"} key consists of a list of objects representing
    the OSPF boundaries, by router ID, OSPF area ID, and the network mask
    of the published summarizing route.

The IP naming scheme for client machines follows the format
    \texttt{"fd14::<AREA ID>:<ROUTER ID>:<CLIENT ID>"}.
Additionally, each \gls{vm} is attached to an IPv4 management network.
The clients utilize static routing, with a default IPv4 route on the management
    interface and a ``fd14::/16'' route on the upstream interface.

% Time sync
For accurate measurement of latency and jitter, synchronized clocks among the
    \glspl{vm} are imperative.
While \gls{ntp} is a common solution for time synchronization, Linux provides
    an alternative mechanism that does not require an \gls{ntp} server or the 
    network stack at all.
Instead, the host injects para-virtualized \gls{ptp} devices into the guests
    facilitating highly accurate time synchronization.
Apart from loading the \texttt{"kvm\_ptp"} Kernel module and configuring
    \textit{``chrony''} as usual, setting the clock source on the host via
    Kernel command line parameters to X was necessary.

% Router, routing protocol,
The router operates on our custom MEADcast Kernel version 6.5.2.
\textit{``Frr''} facilitates the routing stack, which supports a variety of
    routing protocols such as BGP, OSPF, RIP, and PIM \cite{frr_doc}.
OSPF is utilized as the IP Unicast routing protocol and PIM for IP Multicast
    routing.
Beyond the standard setup of \textit{``frr''}, manual alteration of the package
    manager source to \url{https://deb.frrouting.org/} was necessary, as the
    package from the default source misses the IPv6 PIM daemon.
It is noteworthy while OSPF mandates routers to possess only a link-local
    address, the MEADcast discovery phase necessitates each router to have at
    least one global IPv6 address.

\begin{listing}
\begin{minted}[linenos, frame=lines, framesep=2mm, breaklines]{json}
{
    "stub_range": "fd14:0::/32",
    "tran_range": "fd14:0::/32",
    "mgm_gw": "10.10.0.1/8",
    "stubs": [
        {
            "id": 10,
            "mask": 112,
            "area": 1,
            "router": 10,
            "clients": 5
        }, {
            "id": 11,
            "mask": 112,
            "area": 1,
            "router": 10,
            "clients": 5
        }, {
            "id": 20,
            "mask": 112,
            "area": 2,
            "router": 20,
            "clients": 5
        }, {
            "id": 21,
            "mask": 112,
            "area": 2,
            "router": 20,
            "clients": 5
        }
    ],
    "trans": [
        [[  1,   2], 0],  /* [[ <ROUTER ID>, <ROUTER ID>], <OSPF AREA>] */
        [[  1,  10], 1],
        [[  2,  20], 2],
    ],
    "area_boundary": [
        /* { <ROUTER ID>, <OSPF AREA>, <PUBLISHED NET MASK> } */
        { "id": 1, "area": 1, "mask": 96 },
        { "id": 2, "area": 2, "mask": 96 },
    ]
}
\end{minted}
    \caption[Network topology configuration file format]{
        Network topology configuration file format.
        The resulting topology is illustrated in \autoref{fig:ex_topo}
    }
    \label{lst:topo_cfg_format}
\end{listing}

\begin{figure}
    \begin{center}
        \includegraphics[width=0.85\textwidth]{topo_format.pdf}
    \end{center}
    \caption[Example topology]{
        Example topology generated from the configuration file shown in
        \autoref{lst:topo_cfg_format}.
    }
    \label{fig:ex_topo}
\end{figure}

% subsection Technical Infrastructure (end)
% section Experiment (end)

\subsection{Conduction} % (fold)
\label{sub:Conduction}
% How conducted
% How evaluated

% More than 180 experiments, repeated multiple times to ensure validity
% 120 measurements EX1, 50 EX2, 10 EX3
We conducted over 180 measurements across three distinct use cases (UC1-3),
    repeating each measurement multiple times to ensure the validity and
    reliability of the collected data.
Specifically, we conducted more than 120 measurements for UC1, 50 for UC2, and
    10 for UC3.

Each \textit{use case} is delineated by its compilation of routers, transit and
    stub networks, and specific application characteristics, including
    communication and traffic pattern.
An \textit{experiment} entails executing a series of measurements for a given
    use case, featuring a specific number of clients and data volume.
This series comprises measurements for IP Unicast, IP Multicast, and various 
    MEADcast configurations.
A MEADcast configuration is composed of the sender parameterization and the 
    network support level.
For instance, \autoref{tab:usecase_experiment_measurement} provides an excerpt
    of two experiments conducted within UC1 (Live stream).

\begin{table}
\centering
\begin{tabular}{@{}ccccccccccc@{}}
\toprule
& & \multicolumn{3}{c}{\textbf{Experiment}} & \multicolumn{6}{c}{\textbf{MEADcast}} \\
\cmidrule(lr){3-5}\cmidrule(lr){6-11}
& &
\multicolumn{2}{c}{Clients}
& & &
\multicolumn{3}{c}{Discovery} &
\multicolumn{2}{c}{Grouping} \\
\cmidrule(lr){3-4}\cmidrule(lr){7-9}\cmidrule(lr){10-11}

\textbf{\makecell[c]{Use\\case}} &
    Proto.   &
    Tot.      &
    \makecell[c]{per\\Router} &
    \makecell[c]{Vol.\\\itshape (MB/s)}       &
    \makecell[c]{Net.\\Supp.}       &
    \makecell[c]{Dly\\\itshape (s)}    &
    \makecell[c]{Intvl\\\itshape (s)}     &
    \makecell[c]{T. out\\\itshape (s)}    &
    \makecell[c]{Max.\\size}       &
    \makecell[c]{Merge\\range}      \\ \midrule
Stream   & Uni      & 14        & 1         & 1      & -      & -      & -         & -         & -               & -          \\
Stream   & Multi    & 14        & 1         & 1      & -      & -      & -         & -         & -               & -          \\
Stream   & Mead     & 14        & 1         & 1      & 100\%  & 0      & 0         & 0         & 10              & 0          \\
Stream   & Mead     & 14        & 1         & 1      & 100\%  & 0      & 5         & 2         & 10              & 0          \\
\vdots   & \vdots   & \vdots    & \vdots    & \vdots & \vdots & \vdots & \vdots    & \vdots    & \vdots          & \vdots \\[3pt]
Stream   & Mead     & 14        & 1         & 1      & 100\%  & 0      & 0         & 0         & 10              & 2          \\
Stream   & Mead     & 14        & 1         & 1      & L1     & 0      & 0         & 0         & 20              & 0          \\ \midrule
Stream   & Uni      & 28        & 2         & 2.5    & -      & -      & -         & -         & -               & -          \\
Stream   & Multi    & 28        & 2         & 2.5    & -      & -      & -         & -         & -               & -          \\
Stream   & Mead     & 28        & 2         & 2.5    & 100\%  & 0      & 0         & 0         & 10              & 0          \\
Stream   & Mead     & 28        & 2         & 2.5    & 100\%  & 0      & 5         & 2         & 10              & 0          \\
\vdots   & \vdots   & \vdots    & \vdots    & \vdots & \vdots & \vdots & \vdots    & \vdots    & \vdots          & \vdots \\[3pt]
Stream   & Mead     & 28        & 2         & 2.5    & 100\%  & 0      & 0         & 0         & 10              & 2          \\
Stream   & Mead     & 28        & 2         & 2.5    & L1     & 0      & 0         & 0         & 20              & 0          \\
\bottomrule
\end{tabular}
\caption[Use case, experiments and measurements]{
    Use case, experiments and measurements.
    Excerpt of two experiments comprising multiple measurements conducted
    within UC1 (Live stream).
}
\label{tab:usecase_experiment_measurement}
\end{table}

\paragraph{Measurment} % (fold)
\label{par:Measurment}

% Always measured all links. Used ifstat on host (bc each network is just a bridge)
We utilized \textit{``Iperf2''} for the data transmission and to collect
    performance metrics including sender and receiver bandwidth utilization,
    latency, jitter, and packet loss.
The usage of \textit{``Iperf3''} was not feasible due to the necessity of an
    initial \gls{tcp} handshake.

% Used Iperf to measure tx/rx bw utilization, latency, jitter, packet loss
To measure the total network bandwidth utilization, we monitored all links
    within the use case.
As the experiments were conducted in a virtual environment, we simply needed to
    measure the receive and transmission bandwidth of each virtual network,
    which is essentially a network bridge on the host system.
For this purpose, we utilized \textit{``ifstat''}.
% paragraph Measurment (end)

\paragraph{RX/TX} % (fold)
\label{par:RX/TX}
% Uni (1:n):
%   - rx: ssh into all machines, start Iperf server and detach from it
%   - tx: ssh into sender and start one Iperf instance per receiver via gnu parallel
To conduct IP Unicast measurements for UC1 and UC2 we connected to all
    receivers using GNU parallel and SSH, initiated Iperf in server mode and
    detached from the process.
On the sender we initiated one instance of Iperf per receiver using GNU
    parallel.

% Multicast:
%   - rx: same as Unicast (but listen on Multicast address)
%   - tx: start Iperf client on sender with Multicast address
For the IP Multicast measurements, the procedure on the receiver side is
    identical to IP Unicast, except that Iperf listens on a IPv6 Multicast
    address.
On the sender side, one instance of Iperf is started to transmit traffic
    towards all multicast listeners.

% MEADcast:
%   - rx: same as Unicast
%   - tx: start MEADcast sender, and send IPerf into TUN dev
To measure MEADcast, the procedure on the receiver side is identical to IP
    Unicast.
On the sender we started the MEADcast sender implementation and configured
    Iperf to send its traffic into the TUN device, as shown in
    \autoref{fig:tun_dev}.
It is noteworthy, that we initiated the Iperf client (our sender) with the
    \texttt{"--no-udp-fin"} flag to prevent the Iperf server (our receivers)
    from sending a final report to the client, potentially falsifying our
    bandwidth measurements.

% Uni (m:n):
%   - rx: same
%   - tx: ssh into each client and do the same as 1:n
To conduct IP Unicast measurements for UC3 (\gls{p2p}), we employed one
    instance of Iperf in server mode on each client ($n$), along with $n-1$
    instances of Iperf in client mode, sending traffic to each other.
% Multicast (m:n)
%   - not possible, bc Iperf requires receiver to bind to Unicast host to listen to
%   - might be possible if one can bypass this limitation, or
%     start for each sender an Iperf rx
\gls{p2p} Multicast measurements were not feasible because Iperf only supports
    \gls{ssm}.
% MEADcast (m:n)
%   - rx: same as unicast
%   - tx: start MEADcast on each client
The procedure for \gls{p2p} MEADcast measurements is similar to the 1:n
    measurement, except that the MEADcast sender is initiated on each client.
% paragraph RX/TX (end)

% MEADcast
%   - Sender Configuration
%       - UC1-2 1-5 Clients per Router
%       - Max grouping 10, 15, 20, 32
%   - with and without discovery phase
%   - Network support
%       - 100%, L2-L3, L1
\paragraph{MEADcast configuration} % (fold)
\label{par:MEADcast configuration}
In each experiment, we conducted measurements for a variety of MEADcast
    configurations.

To gain insides into both the performance of the data delivery phase itself and
    the implications of the discovery phase, we measured MEADcasts' performance
    with and without considering the discovery phase.
To measure MEADcast's performance without discovery phase, we started the
    sender with a discovery interval of zero, resulting in the sender
    performing a single discovery phase during startup.
After the discovery phase was completed, we conducted our measurement solely
    collecting data from the data delivery phase.
For measuring the performance of MEADcast with discovery phase, we initialized
    the sender with the wait flag set, a delay of zero, a discovery timeout of
    2 seconds, and a discovery interval of 5 seconds.
By setting the wait flag, the sender delays the initial discovery until it
    receives traffic from the TUN device.

Furthermore, we conducted measurements with varying levels of network support,
    encompassing scenarios such as 100\% support across all network layers
    (distribution, access, and core), support solely on the distribution (L2)
    and access layers (L3), support exclusively on the core layer (L1), and
    MEADcast support limited to specific routers.

To explore the performance implications of receiver grouping, we conducted
    measurements with various sender configurations.
These configurations involved different maximum numbers of addresses per
    packet, ranging from 6, 10, 15, and 20, up to 32.
Additionally, the \texttt{"--ok"} parameter was consistently set to 80\% of the
    maximum.
% paragraph MEADcast configuration (end)

% - proved that MEADcast on/off flag has no impact

\paragraph{UC1} % (fold)
\label{par:UC1}
% - As cartesian says 5 Mbit/s for a HD video stream, however since this turned
%   out as unfeasible due to our resource limitations did base line experiment
%   1 Mbit/s audio stream and 2.5 Mbit/s for a SD video stream
% - 1-5 Clients per stub net (total 14-70)
% - For high numbers still sometimes unreliable data
Initially, we conducted experiments for UC1 with a data volume of 5 Mbit/s,
    a common requirement for HD video streams \cite{cartesian_us_bw}.
However, during the measurements, it became evident that our hardware resources
    were insufficient to support this data volume as the group size increased.
Consequently, we adjusted our experiments to use a baseline data volume of 1
    Mbit/s (for audio stream) and 2.5 Mbit/s (for SD video stream)
    \cite{cartesian_us_bw}.
Additionally, we varied the number of receivers per router from 1 to 5.
As a result, we conducted a  total of 10 experiments comprising 14 to 70
    receivers, with data volumes of 1 and 2.5 Mbit/s.
However, even the experiment with 70 clients and a data volume of 2.5 Mbit/s 
    exceeded our resource capacities, resulting in unreliable data.
% paragraph UC1 (end)

\paragraph{UC2} % (fold)
\label{par:UC2}
% - BC iperf has no feature to MAX out bandwidth, went up until we reached
%   packet loss of approx 1.5%
% - started with data volume of 25 MB and 100 MB
% - same relative savings --> omitted 100 MB
To simulate recurring bursts of high traffic volume, we needed to maximize the
    utilization of available bandwidth.
Since \textit{``Iperf''} lacks a feature to send as much data as possible for
    \gls{udp}, we increased the bandwidth parameter until we observed a packet
    loss of approximately 1.5\%.
Initially, we conducted experiments with a data volume of 25 MB and 100 MB.
However, as shown in \autoref{tab:uc2_rel_savings}, the relative savings for 
    25 MB and 100 MB were identical.
Therefore we discontinued the 100 MB experiments.
Subsequently, we conducted experiments with 1, 3, and 5 clients per router
    (totaling 9, 27, 45), with a data volume of 25 MB.
% paragraph UC2 (end)

\paragraph{UC3} % (fold)
\label{par:UC3}
% - To high packet loss already in base experiment
Initially, we conducted an experiment with 12 clients (1 per stub network) and
    a data volume of 1 MB/s, a requirement commonly seen in video conferences
    \cite{cartesian_us_bw}.
However, even in this modest experiment, we observed packet loss rates of
    50\%.
Due to the unreliable data, resulting from our resource limitations we decided
    to discontinue experiments for UC3.
% paragraph UC3 (end)

\paragraph{UC4} % (fold)
\label{par:UC4}
The primary objective of UC4 was to evaluate the potential impact of MEADcast
    processing on latency and jitter.
However, due to the constrained time frame of this thesis and the fact that 
    latency and jitter metrics were collected throughout all previous
    measurements without any discernible impact on jitter, we opted to exclude
    this use case from our study.
% paragraph UC4 (end)

\paragraph{Dynamic network environments} % (fold)
\label{par:Dynamic network environments}
% Firewall
%   - EX3
%   - Sender: c1201
%   - Receivers: c33-36 clients 1-3
%   - Once all routers MEADcast --> MEADcast fallback
%   - Once only r31, r310, and r311 MEADcast --> Unicast fallback
%   - Firewall: Iperf drop IPv6 Routing header on link between r01 r03
%   - Start MEADcast transmission with periodic discovery phase, to ensure
%       firewall starts dropping during MEADcast data delivery
%   - Enable Firewall for 8 seconds
% Routing Change
% Link failure
Dynamic network environments were explored to asses the impact of common
    phenomena such as routing changes, network disruptions, and packet dropping
    by intermediate nodes on MEADcast.
The measurements were conducted within the topology of UC3, with C1201 acting
    as the sender and three receivers located in the stub networks C33-C36
    each.
These effects were consequently induced on the link between R1 and R3.

A route change was simulated by altering the link state on R3's
    \textit{``enp2s0''} interface from ``up'' to ``down''.
This action prompted the underlying routing protocol on R1 to immediately
    switch to an alternative route via R4.

To simulate a link failure, a firewall rule using \textit{``Iptables''} was
    applied on R3 to drop any traffic received on the link towards R1.
The corresponding commands are detailed in \autoref{lst:r3_link_failure}.
Similar to the route alteration experiment, R1 adapted to an alternative route
    via R4.

For simulating packet dropping by an intermediate node, a firewall rule was
    employed on R3 to drop packets containing an IPv6 routing header extension
    on the link towards R1.
First, MEADcast transmission with periodic discovery phase was initiated,
    followed by enabling the firewall rule during MEADcast data transmission.
The associated commands are provided in \autoref{lst:r3_fw}.
After 8 seconds this rule was disabled again, to observe MEADcast reverting to
    its initial grouping.
This experiment was conducted with both 100\% network support and support only
    on L2 and L3 in network domain 3 (R31, R310, R311).
With 100\% network support, the implications of MEADcast falling back to
    another router located in front of the firewall were measured.
Network support only on L2 and L3 in network domain 3, enabled the
    observation of MEADcast falling back to IP Unicast transmission.
% paragraph Dynamic network environments (end)

\paragraph{Anomaly handling} % (fold)
\label{par:Anomaly handling}
% - Injected handcrafted discovery responses via scappy
% - Turn off MEADcast during data transmission
MEADcast's anomaly handling was investigated by intentionally triggering
    anomalies and observing the system's response.
This involved sending manually crafted discovery responses to the sender using
    the \textit{``Scappy''} tool.
Furthermore, we simulated a router's failure to perform MEADcast processing by
    disabeling MEADcast during the data transmission phase.
% paragraph Anomaly handling (end)
% subsection Conduction (end)
